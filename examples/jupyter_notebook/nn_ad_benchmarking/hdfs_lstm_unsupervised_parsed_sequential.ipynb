{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Anomaly Detection on HDFS Dataset using LSTM based models\n",
    "This is a running example of a Log Anomaly Detection on public dataset HDFS. \n",
    "\n",
    "In `logai/applications/openset/anomaly_detection/openset_anomaly_detection_workflow.py` you can find an automated way of running the end-to-end log anomaly detection pipeline on some of the standard open log datasets (HDFS, BGL) on a variety of experimental configurations and using different log anomaly detection models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from logai.applications.openset.anomaly_detection.openset_anomaly_detection_workflow import OpenSetADWorkflowConfig, validate_config_dict\n",
    "from logai.utils.file_utils import read_file\n",
    "from logai.utils.dataset_utils import split_train_dev_test_for_anomaly_detection\n",
    "import logging \n",
    "from logai.dataloader.data_loader import FileDataLoader\n",
    "from logai.preprocess.hdfs_preprocessor import HDFSPreprocessor\n",
    "from logai.information_extraction.log_parser import LogParser\n",
    "from logai.preprocess.openset_partitioner import OpenSetPartitioner\n",
    "from logai.analysis.nn_anomaly_detector import NNAnomalyDetector\n",
    "from logai.information_extraction.log_vectorizer import LogVectorizer\n",
    "from logai.utils import constants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading config from yaml \n",
    "LogAI allows loading config from yaml file. Check out hdfs_lstm_unsupervised_parsed_sequential_config.yaml file to see the entire workflow config \n",
    "\n",
    "This notebook follows the configuration where we apply log parsing, take sequential log features and apply unsupervised anomaly detection using LSTM mdoels. \n",
    "\n",
    "For other configurations(for the same dataset), like parse-free workflow or applying semantic log features or applying supervised anomaly detection or using CNN or Transformer or LogBERT other neural model would work perfectly with this workflow code itself. The only change needed is the config yaml file. Check out some of the other config files provided in the `configs` dir corresponding to those settings \n",
    "- `hdfs_lstm_unsupervised_nonparsed_sequential_config.yaml`\n",
    "- `hdfs_lstm_unsupervised_parsed_semantic_config.yaml`\n",
    "- `hdfs_lstm_supervised_parsed_sequential_config.yaml`\n",
    "- `hdfs_logbert_config.yaml`\n",
    "\n",
    "Since the processing is a little heavy and mostly for illustration purposes, we will also be dumping intermediate outputs of this notebook in the `output_dir` mentioned in the config yaml file (by default it would be a `temp_dir` in this folder). You can purge the directory whenever u need "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"configs/hdfs_lstm_unsupervised_parsed_sequential_config.yaml\"\n",
    "config_parsed = read_file(config_path)\n",
    "config_dict = config_parsed[\"workflow_config\"]\n",
    "validate_config_dict(config_dict)\n",
    "config = OpenSetADWorkflowConfig.from_dict(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data using FileDataLoader \n",
    "FileDataLoader takes file input (.csv or .log or other unspecified format) and loads the data as a LogRecordObject. Most of the remaining components in the pipeline uses this LogRecordObject as input/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Receiving block blk_-1608999687919862906 src: ...\n",
      "1       BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...\n",
      "2       Receiving block blk_-1608999687919862906 src: ...\n",
      "3       Receiving block blk_-1608999687919862906 src: ...\n",
      "4       PacketResponder 1 for block blk_-1608999687919...\n",
      "                              ...                        \n",
      "4514    Deleting block blk_-2126554733521224025 file /...\n",
      "4515    Deleting block blk_-66330728533676520 file /mn...\n",
      "4516    Deleting block blk_872694497849122755 file /mn...\n",
      "4517    Deleting block blk_3947106522258141922 file /m...\n",
      "4518    Deleting block blk_-774246298521956028 file /m...\n",
      "Name: logline, Length: 4519, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataloader = FileDataLoader(config.data_loader_config)\n",
    "logrecord = dataloader.load_data()\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess loaded data using dataset specific Preprocessor (HDFSPreprocessor)\n",
    "Each dataset has its own Preprocessor class (e.g. HDFSPreprocessor for HDFS), which is inherited from from `logai.preprocess.preprocessor.Preprocessor`. \n",
    "\n",
    "Similarly if you want to add your custom dataset you also need to add the CustomPreprocessor object. The main public method of preprocessor is `clean_log` whose input/output is a LogRecordObject\n",
    "\n",
    "This is the only step in the entire pipeline that is dataset-specific. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           Receiving block BLOCK src IP INT dest IP INT \n",
      "1       BLOCK NameSystem.allocateBlock /mnt/hadoop/map...\n",
      "2           Receiving block BLOCK src IP INT dest IP INT \n",
      "3           Receiving block BLOCK src IP INT dest IP INT \n",
      "4         PacketResponder INT for block BLOCK terminating\n",
      "                              ...                        \n",
      "4514    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "4515    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "4516    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "4517    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "4518    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "Name: logline, Length: 4519, dtype: object\n"
     ]
    }
   ],
   "source": [
    "preprocessor = HDFSPreprocessor(config.preprocessor_config, config.label_filepath)\n",
    "preprocessed_filepath = os.path.join(config.output_dir, 'HDFS_5k_processed.csv')            \n",
    "logrecord = preprocessor.clean_log(logrecord)\n",
    "logrecord.save_to_csv(preprocessed_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing unstructured log data using LogParser\n",
    "If the `parse_logline` of OpenSetADWorkflowConfig config object is True, then the preprocessed log data is parsed using the parsing algorithm and hyperparameters mentioned in the confi yaml file \n",
    "\n",
    "The input and output of the LogParser is the LogRecordObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            Receiving block BLOCK src IP INT dest IP INT\n",
      "1       BLOCK NameSystem.allocateBlock /mnt/hadoop/map...\n",
      "2            Receiving block BLOCK src IP INT dest IP INT\n",
      "3            Receiving block BLOCK src IP INT dest IP INT\n",
      "4         PacketResponder INT for block BLOCK terminating\n",
      "                              ...                        \n",
      "4514    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "4515    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "4516    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "4517    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "4518    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "Name: logline, Length: 4519, dtype: object\n"
     ]
    }
   ],
   "source": [
    "parser = LogParser(config.log_parser_config)\n",
    "parsed_result = parser.parse(logrecord.body[constants.LOGLINE_NAME])\n",
    "logrecord.body[constants.LOGLINE_NAME] = parsed_result[constants.PARSED_LOGLINE_NAME]\n",
    "parsed_filepath = os.path.join(config.output_dir, 'HDFS_5k_parsed.csv')\n",
    "logrecord.save_to_csv(parsed_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning or segmenting the log data using the OpenSetPartitioner\n",
    "The next step in the log anomaly detection pipeline involves partitioning the log-lines into segments. \n",
    "\n",
    "This is particularly important for Sequence based Anomaly Detection models which take sequences of loglines as input and learns the representation at the sequence-level in order to detect anomaly patterns at the level of individual loglines as well as log sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/logai/preprocess/partitioner.py:134: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for group_values, data in logrecord_df.groupby(grouper):\n",
      "/Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/logai/preprocess/partitioner.py:134: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for group_values, data in logrecord_df.groupby(grouper):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Receiving block BLOCK src IP INT dest IP INT[S...\n",
      "1       Receiving block BLOCK src IP INT dest IP INT[S...\n",
      "2       BLOCK NameSystem.allocateBlock /mnt/hadoop/map...\n",
      "3       Receiving block BLOCK src IP INT dest IP INT[S...\n",
      "4       Receiving block BLOCK src IP INT dest IP INT[S...\n",
      "                              ...                        \n",
      "3417    IP INT Got exception while serving BLOCK to IP...\n",
      "3418    IP INT Got exception while serving BLOCK to IP...\n",
      "3419    IP INT Got exception while serving BLOCK to IP...\n",
      "3420    IP INT Got exception while serving BLOCK to IP...\n",
      "3421    IP INT Got exception while serving BLOCK to IP...\n",
      "Name: logline, Length: 3422, dtype: object\n"
     ]
    }
   ],
   "source": [
    "partitioner = OpenSetPartitioner(config.open_set_partitioner_config)\n",
    "partitioned_filepath = os.path.join(config.output_dir, 'HDFS_5k_parsed_sliding10.csv')\n",
    "logrecord = partitioner.partition(logrecord)\n",
    "logrecord.save_to_csv(partitioned_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Train, Dev, Test splits\n",
    "This step splits the log dataset in form of a LogRecordObject) and generates LogRecordObject objects for each of train, dev and test splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_train/dev/test:  2242 252 984\n",
      "Train/Dev/Test Anomalous 0 0 424\n",
      "Train/Dev/Test Normal 2242 252 560\n"
     ]
    }
   ],
   "source": [
    "train_filepath = os.path.join(config.output_dir, 'HDFS_5k_parsed_sliding10_unsupervised_train.csv')\n",
    "dev_filepath = os.path.join(config.output_dir, 'HDFS_5k_parsed_sliding10_unsupervised_dev.csv')\n",
    "test_filepath = os.path.join(config.output_dir, 'HDFS_5k_parsed_sliding10_unsupervised_test.csv')\n",
    "\n",
    "(train_data, dev_data, test_data) = split_train_dev_test_for_anomaly_detection(\n",
    "                logrecord,training_type=config.training_type,\n",
    "                test_data_frac_neg_class=config.test_data_frac_neg,\n",
    "                test_data_frac_pos_class=config.test_data_frac_pos,\n",
    "                shuffle=config.train_test_shuffle\n",
    "            )\n",
    "\n",
    "train_data.save_to_csv(train_filepath)\n",
    "dev_data.save_to_csv(dev_filepath)\n",
    "test_data.save_to_csv(test_filepath)\n",
    "print ('Train/Dev/Test Anomalous', len(train_data.labels[train_data.labels[constants.LABELS]==1]), \n",
    "                                   len(dev_data.labels[dev_data.labels[constants.LABELS]==1]), \n",
    "                                   len(test_data.labels[test_data.labels[constants.LABELS]==1]))\n",
    "print ('Train/Dev/Test Normal', len(train_data.labels[train_data.labels[constants.LABELS]==0]), \n",
    "                                   len(dev_data.labels[dev_data.labels[constants.LABELS]==0]), \n",
    "                                   len(test_data.labels[test_data.labels[constants.LABELS]==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Log data into Log Features using LogVectorizer\n",
    "This step transforms the raw log (text) data into machine-readable vectors. \n",
    "\n",
    "For most of the neural anomaly detectors in this library, the exact nature of log vectorization to be applied might be tightly bound to the type of anomaly detector applied (for e.g. for LogBERT model, the corresponding LogBERT vectorizer has to be applied). \n",
    "\n",
    "For the more traditional statistical ML based anomaly detectors (which typically take the input features in form of pandas DataFrame object), you can apply any of the available vectorizers or implement a custom convertor before giving the vectorizer's output to the anomaly detector.\n",
    "\n",
    "For neural anomaly detection models (LSTM, Transformer, CNN) there are two types of LogVectorizers available - Sequential and Semantics which repectively represent log sequences as sequence of log-ids (i.e. ids of loglines) or sequences of token ids.  \n",
    "\n",
    "- `fit` method of logVectorizer is for any tailoring of the logVectorizer model towards the given log training dataset (e.g. a Word2Vec or FastText based logVectorizer model might need to be trained on the given log training data or simply pruning and constructing a vocabulary for the log data from the standard pretrained Word2Vec model\n",
    "\n",
    "- `transform` method of logVectorizer transforms the raw log data (LogRecordObject) to log features (whose data-type is govered by the logVectorizer class, for e.g. logBERT based logVectorizer generates an output of HFDataset (HuggingFace Dataset object) type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = LogVectorizer(config.log_vectorizer_config)\n",
    "vectorizer.fit(train_data)\n",
    "train_features = vectorizer.transform(train_data)\n",
    "dev_features = vectorizer.transform(dev_data)\n",
    "test_features = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Anomaly Detection model on log features using NNAnomalyDetector\n",
    "This step constructs a neural anomaly detector model (for e.g. lstm based sequence anomaly detector which is a type of a Forecast based neural anomaly detector. It can be used in two modes \n",
    "\n",
    "- Primarily as an unsupervised learning model, where it learns to forecast the id of the next log-line, given an input log-sequence. During inference, based on the model's confidence in the forecasting for a given log sequence, the model infers whether the input is anomalous or not. In the unsupervised mode, it assumes that the training data is entirely consisting of normal log sequence. Presence of any anomalous log sequence is permissible but will degrade the model performance\n",
    "\n",
    "- It can also be used in the standard supervised learning mode, where given a log sequence as input, the model learns to directly predict the label (anomalous or not). In this mode the model needs to see both normal and anomalous logs during training. In many situations the supervised model can actually perform somewhat worser than unsupervised version, because of overfitting or bias towards anomaly patterns seen during training, especially if there are only rare occurrences of anomalies in the train data.\n",
    "\n",
    "In this example we are using the unsupervised mode of the LSTM based Anomaly Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start training on 561 batches with cpu.\n",
      "INFO:root:Batch 100, training loss : 2.5444133508205415\n",
      "INFO:root:Batch 200, training loss : 2.2127846387028693\n",
      "INFO:root:Batch 300, training loss : 1.8919825654228528\n",
      "INFO:root:Batch 400, training loss : 1.657002859003842\n",
      "INFO:root:Batch 500, training loss : 1.4728448408097028\n",
      "INFO:root:Epoch 1/10, training loss: 1.3847237395225573 [4.745373964309692s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.6193237191154843\n",
      "INFO:root:Dev acc @ top-1: 0.8611111111111112  correct: 217 out of 252\n",
      "INFO:root:Saving model to temp_output/HDFS_5k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 0.8119356613606215\n",
      "INFO:root:Batch 200, training loss : 0.6999477495253086\n",
      "INFO:root:Batch 300, training loss : 0.6303683702647686\n",
      "INFO:root:Batch 400, training loss : 0.5877456868812442\n",
      "INFO:root:Batch 500, training loss : 0.5587526946216822\n",
      "INFO:root:Epoch 2/10, training loss: 0.5479684943422914 [5.022781848907471s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.4233089229535489\n",
      "INFO:root:Dev acc @ top-1: 0.9087301587301587  correct: 229 out of 252\n",
      "INFO:root:Saving model to temp_output/HDFS_5k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 0.6062310394644738\n",
      "INFO:root:Batch 200, training loss : 0.5271046921983361\n",
      "INFO:root:Batch 300, training loss : 0.47705612365156413\n",
      "INFO:root:Batch 400, training loss : 0.45219052989035846\n",
      "INFO:root:Batch 500, training loss : 0.43710758490115403\n",
      "INFO:root:Epoch 3/10, training loss: 0.43230435836548897 [4.793680906295776s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.35824570526915883\n",
      "INFO:root:Dev acc @ top-1: 0.9087301587301587  correct: 229 out of 252\n",
      "INFO:root:Saving model to temp_output/HDFS_5k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 0.5593119919300079\n",
      "INFO:root:Batch 200, training loss : 0.4758040377032012\n",
      "INFO:root:Batch 300, training loss : 0.4271607056384285\n",
      "INFO:root:Batch 400, training loss : 0.4065205448260531\n",
      "INFO:root:Batch 500, training loss : 0.39439637122303245\n",
      "INFO:root:Epoch 4/10, training loss: 0.39033514947037806 [4.340764284133911s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.323716876170938\n",
      "INFO:root:Dev acc @ top-1: 0.9166666666666666  correct: 231 out of 252\n",
      "INFO:root:Saving model to temp_output/HDFS_5k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 0.5313686485216021\n",
      "INFO:root:Batch 200, training loss : 0.44698535192757843\n",
      "INFO:root:Batch 300, training loss : 0.3997389906148116\n",
      "INFO:root:Batch 400, training loss : 0.3815726961614564\n",
      "INFO:root:Batch 500, training loss : 0.37094254630804063\n",
      "INFO:root:Epoch 5/10, training loss: 0.36707610739710445 [4.556123971939087s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.3026235353733812\n",
      "INFO:root:Dev acc @ top-1: 0.9206349206349206  correct: 232 out of 252\n",
      "INFO:root:Saving model to temp_output/HDFS_5k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 0.5098098189942539\n",
      "INFO:root:Batch 200, training loss : 0.4273931292351335\n",
      "INFO:root:Batch 300, training loss : 0.3817810482283433\n",
      "INFO:root:Batch 400, training loss : 0.36532224328955637\n",
      "INFO:root:Batch 500, training loss : 0.3557395319119096\n",
      "INFO:root:Epoch 6/10, training loss: 0.3520175537008148 [4.255572319030762s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.2888648943413818\n",
      "INFO:root:Dev acc @ top-1: 0.9206349206349206  correct: 232 out of 252\n",
      "INFO:root:Saving model to temp_output/HDFS_5k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 0.4917521276511252\n",
      "INFO:root:Batch 200, training loss : 0.412737078233622\n",
      "INFO:root:Batch 300, training loss : 0.3687833446978281\n",
      "INFO:root:Batch 400, training loss : 0.35361984559101983\n",
      "INFO:root:Batch 500, training loss : 0.34488276604004203\n",
      "INFO:root:Epoch 7/10, training loss: 0.34126932778912006 [4.576394081115723s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.2789851903678879\n",
      "INFO:root:Dev acc @ top-1: 0.9206349206349206  correct: 232 out of 252\n",
      "INFO:root:Saving model to temp_output/HDFS_5k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 0.4761390234902501\n",
      "INFO:root:Batch 200, training loss : 0.4009218687051907\n",
      "INFO:root:Batch 300, training loss : 0.35853197876984877\n",
      "INFO:root:Batch 400, training loss : 0.34441839063889346\n",
      "INFO:root:Batch 500, training loss : 0.33640738629736006\n",
      "INFO:root:Epoch 8/10, training loss: 0.3328344969322935 [4.471644878387451s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.2709775109376226\n",
      "INFO:root:Dev acc @ top-1: 0.9206349206349206  correct: 232 out of 252\n",
      "INFO:root:Saving model to temp_output/HDFS_5k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 0.46234940515831113\n",
      "INFO:root:Batch 200, training loss : 0.3908668108377606\n",
      "INFO:root:Batch 300, training loss : 0.3499062396259978\n",
      "INFO:root:Batch 400, training loss : 0.336650931002805\n",
      "INFO:root:Batch 500, training loss : 0.32919703105743975\n",
      "INFO:root:Epoch 9/10, training loss: 0.3255563504647782 [4.691821813583374s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.26366612406831885\n",
      "INFO:root:Dev acc @ top-1: 0.9206349206349206  correct: 232 out of 252\n",
      "INFO:root:Saving model to temp_output/HDFS_5k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 0.449552088920027\n",
      "INFO:root:Batch 200, training loss : 0.38197577288374307\n",
      "INFO:root:Batch 300, training loss : 0.3424138610313336\n",
      "INFO:root:Batch 400, training loss : 0.3298432907636743\n",
      "INFO:root:Batch 500, training loss : 0.32276765487808734\n",
      "INFO:root:Epoch 10/10, training loss: 0.31892499176153677 [4.461452960968018s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.2566168014374044\n",
      "INFO:root:Dev acc @ top-1: 0.9206349206349206  correct: 232 out of 252\n",
      "INFO:root:Saving model to temp_output/HDFS_5k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Loading model from temp_output/HDFS_5k_parsed_session_supervised_AD/model_lstm/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "anomaly_detector = NNAnomalyDetector(config=config.nn_anomaly_detection_config)\n",
    "anomaly_detector.fit(train_features, dev_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Anomalous Log Sequences using the trained NNAnomalyDetector \n",
    "This step simply runs the predict method of the NNAnomalyDetctor, which takes the test log features and returns the prediction results. \n",
    "\n",
    "Currently the prediction results are raw python objects - For e.g. in this case, it returns a dict object which maintains some of the standard self-explanatory metrics (like F1, precision, recall) as well as a vector of instance-wise predictions and true labels. For other NNAnomalyDetector models, the data-type of the predict_results object might be different (for eg. for LogBERT model, the predict_results object returned is a pandas DataFrame consisting of the prediction results and evaluation metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating test data.\n",
      "INFO:root:Finish inference. [0.36537694931030273s]\n",
      "INFO:root:Calculating acc sum.\n",
      "INFO:root:Finish generating store_df.\n",
      "INFO:root:Finish counting [0.06994795799255371s]\n",
      "INFO:root:Best result: f1: 1.0 rc: 1.0 pc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 1.0, 'rc': 1.0, 'pc': 1.0, 'pred': 0     1\n",
      "1     1\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    1\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "20    0\n",
      "21    0\n",
      "22    0\n",
      "23    0\n",
      "24    1\n",
      "25    1\n",
      "26    0\n",
      "27    1\n",
      "28    1\n",
      "29    0\n",
      "Name: window_pred_anomaly_10, dtype: int64, 'true': 0     1\n",
      "1     1\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    1\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "20    0\n",
      "21    0\n",
      "22    0\n",
      "23    0\n",
      "24    1\n",
      "25    1\n",
      "26    0\n",
      "27    1\n",
      "28    1\n",
      "29    0\n",
      "Name: window_anomalies, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "predict_results = anomaly_detector.predict(test_features)\n",
    "print (predict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
