{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Anomaly Detection on BGL Dataset using LSTM based models\n",
    "This is a running example of an end-to-end workflow for Log Anomaly Detection on public dataset BGL using LSTM based neural anomaly detectors.\n",
    "\n",
    "It is very similar to that on the HDFS dataset. We will only mark out the dataset specific portions of this workflow i.e. the parts that differ between the two datasets. \n",
    "\n",
    "For a more complete elaboration of the full workflow please refer to `hdfs_lstm_unsupervised_parsed_sequential.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from logai.applications.openset.anomaly_detection.openset_anomaly_detection_workflow import OpenSetADWorkflowConfig, validate_config_dict\n",
    "from logai.utils.file_utils import read_file\n",
    "from logai.utils.dataset_utils import split_train_dev_test_for_anomaly_detection\n",
    "import logging \n",
    "from logai.dataloader.data_loader import FileDataLoader\n",
    "from logai.preprocess.bgl_preprocessor import BGLPreprocessor\n",
    "from logai.information_extraction.log_parser import LogParser\n",
    "from logai.preprocess.openset_partitioner import OpenSetPartitioner\n",
    "from logai.analysis.nn_anomaly_detector import NNAnomalyDetector\n",
    "from logai.information_extraction.log_vectorizer import LogVectorizer\n",
    "from logai.utils import constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading config from yaml\n",
    "While the way to load config from yaml file is generic across all datasets, dive into the yaml file itself to specify particular nuances of your dataset (for e.g. regex patterns or mapping of column names to the LogRecordObject attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"configs/bgl_lstm_unsupervised_parsed_sequential_config.yaml\"\n",
    "config_parsed = read_file(config_path)\n",
    "config_dict = config_parsed[\"workflow_config\"]\n",
    "validate_config_dict(config_dict)\n",
    "config = OpenSetADWorkflowConfig.from_dict(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         RAS KERNEL INFO instruction cache parity error...\n",
      "1         RAS KERNEL INFO instruction cache parity error...\n",
      "2         RAS KERNEL INFO instruction cache parity error...\n",
      "3         RAS KERNEL INFO instruction cache parity error...\n",
      "4         RAS KERNEL INFO instruction cache parity error...\n",
      "                                ...                        \n",
      "358455    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358456    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358457    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358458    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358459    RAS KERNEL FATAL idoproxy communication failur...\n",
      "Name: logline, Length: 358460, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/logai/dataloader/data_loader.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected[constants.LOG_TIMESTAMPS] = pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "dataloader = FileDataLoader(config.data_loader_config)\n",
    "logrecord = dataloader.load_data()\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing loaded data using BGLPreprocessor \n",
    "This is the only part of the workflow that differs based on the dataset used. Each dataset must have its own Preprocessor class implemented. The main functionalities of the preprocessor class is to help process the raw log data and extract the standard fields of the LogRecordObject (e.g. body, labels, timestamps, span_ids, attributes etc). \n",
    "\n",
    "For some of the fields (like timestamp) where the extraction is generic, it is already automatically handled in the DataLoader class. \n",
    "\n",
    "Whereas, for some of the more dataset-specific fields (e.g. span_ids or labels), the custom extraction code has to be implemented in the dataset's corresponding Preprocessor class. For e.g. raw BGL dataset does not have any id associated with the loglines. But most existing log anomaly detection literature does a fixed time-partitioning of the logs and uses these partition indices as ids of the log segments. \n",
    "\n",
    "If you want to use a different time-partitioning or a different scheme for id-ing the loglines in BGL dataset, you have to write your own custom Preprocessor for BGL to serve that purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         RAS KERNEL INFO instruction cache parity error...\n",
      "1         RAS KERNEL INFO instruction cache parity error...\n",
      "2         RAS KERNEL INFO instruction cache parity error...\n",
      "3         RAS KERNEL INFO instruction cache parity error...\n",
      "4         RAS KERNEL INFO instruction cache parity error...\n",
      "                                ...                        \n",
      "358455    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358456    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358457    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358458    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358459    RAS KERNEL FATAL idoproxy communication failur...\n",
      "Name: logline, Length: 358460, dtype: object\n"
     ]
    }
   ],
   "source": [
    "preprocessor = BGLPreprocessor(config.preprocessor_config)\n",
    "preprocessed_filepath = os.path.join(config.output_dir, 'BGL_11k_processed.csv')            \n",
    "logrecord = preprocessor.clean_log(logrecord)\n",
    "logrecord.save_to_csv(preprocessed_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         RAS KERNEL INFO instruction cache parity error...\n",
      "1         RAS KERNEL INFO instruction cache parity error...\n",
      "2         RAS KERNEL INFO instruction cache parity error...\n",
      "3         RAS KERNEL INFO instruction cache parity error...\n",
      "4         RAS KERNEL INFO instruction cache parity error...\n",
      "                                ...                        \n",
      "358455    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358456    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358457    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358458    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358459    RAS KERNEL FATAL idoproxy communication failur...\n",
      "Name: logline, Length: 358460, dtype: object\n"
     ]
    }
   ],
   "source": [
    "parser = LogParser(config.log_parser_config)\n",
    "parsed_result = parser.parse(logrecord.body[constants.LOGLINE_NAME])\n",
    "logrecord.body[constants.LOGLINE_NAME] = parsed_result[constants.PARSED_LOGLINE_NAME]\n",
    "parsed_filepath = os.path.join(config.output_dir, 'BGL_11k_parsed.csv')\n",
    "logrecord.save_to_csv(parsed_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/logai/preprocess/partitioner.py:134: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for group_values, data in logrecord_df.groupby(grouper):\n",
      "/Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/logai/preprocess/partitioner.py:134: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for group_values, data in logrecord_df.groupby(grouper):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         RAS KERNEL INFO instruction cache parity error...\n",
      "1         RAS KERNEL INFO instruction cache parity error...\n",
      "2         RAS KERNEL INFO instruction cache parity error...\n",
      "3         RAS KERNEL INFO instruction cache parity error...\n",
      "4         RAS KERNEL INFO instruction cache parity error...\n",
      "                                ...                        \n",
      "346567    RAS KERNEL FATAL Lustre mount FAILED ALPHANUM ...\n",
      "346568    RAS KERNEL FATAL Lustre mount FAILED ALPHANUM ...\n",
      "346569    RAS KERNEL FATAL Lustre mount FAILED ALPHANUM ...\n",
      "346570    RAS KERNEL FATAL Lustre mount FAILED ALPHANUM ...\n",
      "346571    RAS KERNEL FATAL idoproxy communication failur...\n",
      "Name: logline, Length: 346572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "partitioner = OpenSetPartitioner(config.open_set_partitioner_config)\n",
    "partitioned_filepath = os.path.join(config.output_dir, 'BGL_11k_parsed_sliding10.csv')\n",
    "logrecord = partitioner.partition(logrecord)\n",
    "logrecord.save_to_csv(partitioned_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_train/dev/test:  9503 1941 336941\n",
      "Train/Dev/Test Anomalous 0 0 336941\n",
      "Train/Dev/Test Normal 9503 1941 0\n"
     ]
    }
   ],
   "source": [
    "train_filepath = os.path.join(config.output_dir, 'BGL_11k_parsed_sliding10_unsupervised_train.csv')\n",
    "dev_filepath = os.path.join(config.output_dir, 'BGL_11k_parsed_sliding10_unsupervised_dev.csv')\n",
    "test_filepath = os.path.join(config.output_dir, 'BGL_11k_parsed_sliding10_unsupervised_test.csv')\n",
    "\n",
    "(train_data, dev_data, test_data) = split_train_dev_test_for_anomaly_detection(\n",
    "                logrecord,training_type=config.training_type,\n",
    "                test_data_frac_neg_class=config.test_data_frac_neg,\n",
    "                test_data_frac_pos_class=config.test_data_frac_pos,\n",
    "                shuffle=config.train_test_shuffle\n",
    "            )\n",
    "\n",
    "train_data.save_to_csv(train_filepath)\n",
    "dev_data.save_to_csv(dev_filepath)\n",
    "test_data.save_to_csv(test_filepath)\n",
    "print ('Train/Dev/Test Anomalous', len(train_data.labels[train_data.labels[constants.LABELS]==1]), \n",
    "                                   len(dev_data.labels[dev_data.labels[constants.LABELS]==1]), \n",
    "                                   len(test_data.labels[test_data.labels[constants.LABELS]==1]))\n",
    "print ('Train/Dev/Test Normal', len(train_data.labels[train_data.labels[constants.LABELS]==0]), \n",
    "                                   len(dev_data.labels[dev_data.labels[constants.LABELS]==0]), \n",
    "                                   len(test_data.labels[test_data.labels[constants.LABELS]==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = LogVectorizer(config.log_vectorizer_config)\n",
    "vectorizer.fit(train_data)\n",
    "train_features = vectorizer.transform(train_data)\n",
    "dev_features = vectorizer.transform(dev_data)\n",
    "test_features = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start training on 2376 batches with cpu.\n",
      "INFO:root:Batch 100, training loss : 1.0679804325103759\n",
      "INFO:root:Batch 200, training loss : 0.5701437311060726\n",
      "INFO:root:Batch 300, training loss : 0.3870733882921437\n",
      "INFO:root:Batch 400, training loss : 0.29765536113176494\n",
      "INFO:root:Batch 500, training loss : 0.23961709387600422\n",
      "INFO:root:Batch 600, training loss : 0.20054602410256242\n",
      "INFO:root:Batch 700, training loss : 0.202668375275763\n",
      "INFO:root:Batch 800, training loss : 0.17807484669901896\n",
      "INFO:root:Batch 900, training loss : 0.158748176471175\n",
      "INFO:root:Batch 1000, training loss : 0.14319151785061696\n",
      "INFO:root:Batch 1100, training loss : 0.13040706076151268\n",
      "INFO:root:Batch 1200, training loss : 0.16973540704445136\n",
      "INFO:root:Batch 1300, training loss : 0.20512259510614408\n",
      "INFO:root:Batch 1400, training loss : 0.1916192350408528\n",
      "INFO:root:Batch 1500, training loss : 0.17939882093776638\n",
      "INFO:root:Batch 1600, training loss : 0.1685270083033538\n",
      "INFO:root:Batch 1700, training loss : 0.15884647699771448\n",
      "INFO:root:Batch 1800, training loss : 0.1501909039745159\n",
      "INFO:root:Batch 1900, training loss : 0.14241429303588934\n",
      "INFO:root:Batch 2000, training loss : 0.13539366200822406\n",
      "INFO:root:Batch 2100, training loss : 0.12902637960444693\n",
      "INFO:root:Batch 2200, training loss : 0.12322676905928265\n",
      "INFO:root:Batch 2300, training loss : 0.11792305412636463\n",
      "INFO:root:Epoch 1/10, training loss: 0.11418634861634584 [16.017497062683105s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.14657151969623994\n",
      "INFO:root:Dev acc @ top-1: 0.949510561566203  correct: 1843 out of 1941\n",
      "INFO:root:Saving model to temp_output/BGL_11k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 0.0004213829807849834\n",
      "INFO:root:Batch 200, training loss : 0.0002443603327992605\n",
      "INFO:root:Batch 300, training loss : 0.0001731058765714503\n",
      "INFO:root:Batch 400, training loss : 0.008327940694575773\n",
      "INFO:root:Batch 500, training loss : 0.006667871630979789\n",
      "INFO:root:Batch 600, training loss : 0.005560423220637555\n",
      "INFO:root:Batch 700, training loss : 0.04219940363629056\n",
      "INFO:root:Batch 800, training loss : 0.03705547887882858\n",
      "INFO:root:Batch 900, training loss : 0.032984191898722204\n",
      "INFO:root:Batch 1000, training loss : 0.029714093870279613\n",
      "INFO:root:Batch 1100, training loss : 0.027032953813706462\n",
      "INFO:root:Batch 1200, training loss : 0.04416153596241202\n",
      "INFO:root:Batch 1300, training loss : 0.053515805331878184\n",
      "INFO:root:Batch 1400, training loss : 0.049729166470517935\n",
      "INFO:root:Batch 1500, training loss : 0.04643789040330254\n",
      "INFO:root:Batch 1600, training loss : 0.04355284154782566\n",
      "INFO:root:Batch 1700, training loss : 0.04100400989017491\n",
      "INFO:root:Batch 1800, training loss : 0.03873624734692081\n",
      "INFO:root:Batch 1900, training loss : 0.03670569392867803\n",
      "INFO:root:Batch 2000, training loss : 0.03487709481630736\n",
      "INFO:root:Batch 2100, training loss : 0.03322181812385333\n",
      "INFO:root:Batch 2200, training loss : 0.03171637855458936\n",
      "INFO:root:Batch 2300, training loss : 0.030341340390672406\n",
      "INFO:root:Epoch 2/10, training loss: 0.029373441960175873 [16.960505962371826s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.13852181444859907\n",
      "INFO:root:Dev acc @ top-1: 0.949510561566203  correct: 1843 out of 1941\n",
      "INFO:root:Saving model to temp_output/BGL_11k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 2.3092535566320295e-05\n",
      "INFO:root:Batch 200, training loss : 1.3601431455754208e-05\n",
      "INFO:root:Batch 300, training loss : 9.747113204336225e-06\n",
      "INFO:root:Batch 400, training loss : 0.009283773351008336\n",
      "INFO:root:Batch 500, training loss : 0.007427795447029439\n",
      "INFO:root:Batch 600, training loss : 0.006190345118565119\n",
      "INFO:root:Batch 700, training loss : 0.03823295157282636\n",
      "INFO:root:Batch 800, training loss : 0.03367904206130731\n",
      "INFO:root:Batch 900, training loss : 0.030010716312199797\n",
      "INFO:root:Batch 1000, training loss : 0.027051624262883253\n",
      "INFO:root:Batch 1100, training loss : 0.0246201624971346\n",
      "INFO:root:Batch 1200, training loss : 0.03492692081487159\n",
      "INFO:root:Batch 1300, training loss : 0.04374083986775321\n",
      "INFO:root:Batch 1400, training loss : 0.04062131729292657\n",
      "INFO:root:Batch 1500, training loss : 0.03791731608094627\n",
      "INFO:root:Batch 1600, training loss : 0.035551001220451045\n",
      "INFO:root:Batch 1700, training loss : 0.03346282391404656\n",
      "INFO:root:Batch 1800, training loss : 0.03160645743762441\n",
      "INFO:root:Batch 1900, training loss : 0.02994532254102308\n",
      "INFO:root:Batch 2000, training loss : 0.028450149864076367\n",
      "INFO:root:Batch 2100, training loss : 0.027097243460014915\n",
      "INFO:root:Batch 2200, training loss : 0.02586721698242214\n",
      "INFO:root:Batch 2300, training loss : 0.024744047538192718\n",
      "INFO:root:Epoch 3/10, training loss: 0.02395360981905995 [16.13777804374695s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.12849342489215565\n",
      "INFO:root:Dev acc @ top-1: 0.949510561566203  correct: 1843 out of 1941\n",
      "INFO:root:Saving model to temp_output/BGL_11k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 1.8722059335232187e-05\n",
      "INFO:root:Batch 200, training loss : 1.0328412080298222e-05\n",
      "INFO:root:Batch 300, training loss : 7.258732918368575e-06\n",
      "INFO:root:Batch 400, training loss : 0.009534109291524828\n",
      "INFO:root:Batch 500, training loss : 0.007628855982814457\n",
      "INFO:root:Batch 600, training loss : 0.006358306040580525\n",
      "INFO:root:Batch 700, training loss : 0.03463204516649317\n",
      "INFO:root:Batch 800, training loss : 0.030552937189081603\n",
      "INFO:root:Batch 900, training loss : 0.02724488201559628\n",
      "INFO:root:Batch 1000, training loss : 0.024571623751300647\n",
      "INFO:root:Batch 1100, training loss : 0.022372486515500483\n",
      "INFO:root:Batch 1200, training loss : 0.030288031668984937\n",
      "INFO:root:Batch 1300, training loss : 0.038328956223600615\n",
      "INFO:root:Batch 1400, training loss : 0.03559331094162839\n",
      "INFO:root:Batch 1500, training loss : 0.03322229383355807\n",
      "INFO:root:Batch 1600, training loss : 0.03114755440144336\n",
      "INFO:root:Batch 1700, training loss : 0.02931682251391655\n",
      "INFO:root:Batch 1800, training loss : 0.027689431306374294\n",
      "INFO:root:Batch 1900, training loss : 0.026233283578398856\n",
      "INFO:root:Batch 2000, training loss : 0.02492269578817104\n",
      "INFO:root:Batch 2100, training loss : 0.02373687831353631\n",
      "INFO:root:Batch 2200, training loss : 0.02265881832094816\n",
      "INFO:root:Batch 2300, training loss : 0.02167446364820179\n",
      "INFO:root:Epoch 4/10, training loss: 0.02098174428071236 [15.971168041229248s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.11424520496340262\n",
      "INFO:root:Dev acc @ top-1: 0.949510561566203  correct: 1843 out of 1941\n",
      "INFO:root:Saving model to temp_output/BGL_11k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 2.436353424400295e-05\n",
      "INFO:root:Batch 200, training loss : 1.3016827462024593e-05\n",
      "INFO:root:Batch 300, training loss : 8.971537062810361e-06\n",
      "INFO:root:Batch 400, training loss : 0.0095826182745472\n",
      "INFO:root:Batch 500, training loss : 0.0076688976865268614\n",
      "INFO:root:Batch 600, training loss : 0.006392179968152618\n",
      "INFO:root:Batch 700, training loss : 0.03250207468351018\n",
      "INFO:root:Batch 800, training loss : 0.0287074193765811\n",
      "INFO:root:Batch 900, training loss : 0.025612690186497453\n",
      "INFO:root:Batch 1000, training loss : 0.02310727022774006\n",
      "INFO:root:Batch 1100, training loss : 0.0210441933736021\n",
      "INFO:root:Batch 1200, training loss : 0.02802199205526326\n",
      "INFO:root:Batch 1300, training loss : 0.035060872236361305\n",
      "INFO:root:Batch 1400, training loss : 0.03255775904673385\n",
      "INFO:root:Batch 1500, training loss : 0.030388338572877045\n",
      "INFO:root:Batch 1600, training loss : 0.028490058107933224\n",
      "INFO:root:Batch 1700, training loss : 0.0268150610003583\n",
      "INFO:root:Batch 1800, training loss : 0.02532614680121109\n",
      "INFO:root:Batch 1900, training loss : 0.023993929606733015\n",
      "INFO:root:Batch 2000, training loss : 0.02279491070740977\n",
      "INFO:root:Batch 2100, training loss : 0.021710057858515753\n",
      "INFO:root:Batch 2200, training loss : 0.02072380480455665\n",
      "INFO:root:Batch 2300, training loss : 0.019823298217976345\n",
      "INFO:root:Epoch 5/10, training loss: 0.019189594991916983 [16.1480610370636s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.09393441482256161\n",
      "INFO:root:Dev acc @ top-1: 0.9958784131890778  correct: 1933 out of 1941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saving model to temp_output/BGL_11k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 2.709773368678725e-05\n",
      "INFO:root:Batch 200, training loss : 1.4377370671923018e-05\n",
      "INFO:root:Batch 300, training loss : 9.85154512193276e-06\n",
      "INFO:root:Batch 400, training loss : 0.009547102284103205\n",
      "INFO:root:Batch 500, training loss : 0.007640721163722105\n",
      "INFO:root:Batch 600, training loss : 0.0063690194076878965\n",
      "INFO:root:Batch 700, training loss : 0.029735242201632397\n",
      "INFO:root:Batch 800, training loss : 0.026274489179851698\n",
      "INFO:root:Batch 900, training loss : 0.023439274210898083\n",
      "INFO:root:Batch 1000, training loss : 0.02114173855920791\n",
      "INFO:root:Batch 1100, training loss : 0.019248547855268302\n",
      "INFO:root:Batch 1200, training loss : 0.025742144560937135\n",
      "INFO:root:Batch 1300, training loss : 0.03193830742421688\n",
      "INFO:root:Batch 1400, training loss : 0.02965785890426125\n",
      "INFO:root:Batch 1500, training loss : 0.027681440384995655\n",
      "INFO:root:Batch 1600, training loss : 0.02595205190364272\n",
      "INFO:root:Batch 1700, training loss : 0.024426096277759938\n",
      "INFO:root:Batch 1800, training loss : 0.0230696736606975\n",
      "INFO:root:Batch 1900, training loss : 0.021856021893969665\n",
      "INFO:root:Batch 2000, training loss : 0.020763715872992323\n",
      "INFO:root:Batch 2100, training loss : 0.019775421229187188\n",
      "INFO:root:Batch 2200, training loss : 0.018876958331729553\n",
      "INFO:root:Batch 2300, training loss : 0.018056614764646844\n",
      "INFO:root:Epoch 6/10, training loss: 0.017479326238895147 [15.93640685081482s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.07032109093010067\n",
      "INFO:root:Dev acc @ top-1: 0.9958784131890778  correct: 1933 out of 1941\n",
      "INFO:root:Saving model to temp_output/BGL_11k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 2.1526681503019064e-05\n",
      "INFO:root:Batch 200, training loss : 1.1581711813732909e-05\n",
      "INFO:root:Batch 300, training loss : 8.001680287937536e-06\n",
      "INFO:root:Batch 400, training loss : 0.009499397899747351\n",
      "INFO:root:Batch 500, training loss : 0.007602826587830805\n",
      "INFO:root:Batch 600, training loss : 0.0063374992050243386\n",
      "INFO:root:Batch 700, training loss : 0.027369869269035202\n",
      "INFO:root:Batch 800, training loss : 0.02418449257556091\n",
      "INFO:root:Batch 900, training loss : 0.021569374640281087\n",
      "INFO:root:Batch 1000, training loss : 0.01945022061793975\n",
      "INFO:root:Batch 1100, training loss : 0.01770760903188623\n",
      "INFO:root:Batch 1200, training loss : 0.02378689216197112\n",
      "INFO:root:Batch 1300, training loss : 0.029386256915987324\n",
      "INFO:root:Batch 1400, training loss : 0.027287937383295328\n",
      "INFO:root:Batch 1500, training loss : 0.025469373443447844\n",
      "INFO:root:Batch 1600, training loss : 0.023878108464119946\n",
      "INFO:root:Batch 1700, training loss : 0.02247403247681779\n",
      "INFO:root:Batch 1800, training loss : 0.021225951885891162\n",
      "INFO:root:Batch 1900, training loss : 0.020109235149104975\n",
      "INFO:root:Batch 2000, training loss : 0.019104178820805486\n",
      "INFO:root:Batch 2100, training loss : 0.01819482902989843\n",
      "INFO:root:Batch 2200, training loss : 0.017368140195112677\n",
      "INFO:root:Batch 2300, training loss : 0.016613331437371193\n",
      "INFO:root:Epoch 7/10, training loss: 0.016082165592825347 [15.927229881286621s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.051126134314977714\n",
      "INFO:root:Dev acc @ top-1: 0.9958784131890778  correct: 1933 out of 1941\n",
      "INFO:root:Saving model to temp_output/BGL_11k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 1.5032714595690776e-05\n",
      "INFO:root:Batch 200, training loss : 7.893654556028196e-06\n",
      "INFO:root:Batch 300, training loss : 5.395156020995273e-06\n",
      "INFO:root:Batch 400, training loss : 0.009692216746726387\n",
      "INFO:root:Batch 500, training loss : 0.007757717277138682\n",
      "INFO:root:Batch 600, training loss : 0.006466862269117541\n",
      "INFO:root:Batch 700, training loss : 0.024276560081939187\n",
      "INFO:root:Batch 800, training loss : 0.021398118604313794\n",
      "INFO:root:Batch 900, training loss : 0.01907711192403882\n",
      "INFO:root:Batch 1000, training loss : 0.01720283961477469\n",
      "INFO:root:Batch 1100, training loss : 0.015661774160962833\n",
      "INFO:root:Batch 1200, training loss : 0.021496500062235385\n",
      "INFO:root:Batch 1300, training loss : 0.026837858196939107\n",
      "INFO:root:Batch 1400, training loss : 0.024921452704316817\n",
      "INFO:root:Batch 1500, training loss : 0.02326055141381547\n",
      "INFO:root:Batch 1600, training loss : 0.021807253694987806\n",
      "INFO:root:Batch 1700, training loss : 0.020524918925208205\n",
      "INFO:root:Batch 1800, training loss : 0.019385058047970524\n",
      "INFO:root:Batch 1900, training loss : 0.018365166025282095\n",
      "INFO:root:Batch 2000, training loss : 0.01744725360857336\n",
      "INFO:root:Batch 2100, training loss : 0.016616749614713357\n",
      "INFO:root:Batch 2200, training loss : 0.01586173866886358\n",
      "INFO:root:Batch 2300, training loss : 0.015172376443200917\n",
      "INFO:root:Epoch 8/10, training loss: 0.014687266832011676 [15.92223072052002s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.04007332314838241\n",
      "INFO:root:Dev acc @ top-1: 0.9958784131890778  correct: 1933 out of 1941\n",
      "INFO:root:Saving model to temp_output/BGL_11k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 1.3574954723480914e-05\n",
      "INFO:root:Batch 200, training loss : 7.254777547984758e-06\n",
      "INFO:root:Batch 300, training loss : 5.004206057227142e-06\n",
      "INFO:root:Batch 400, training loss : 0.009558644948392824\n",
      "INFO:root:Batch 500, training loss : 0.007650755413913601\n",
      "INFO:root:Batch 600, training loss : 0.0063778299017820455\n",
      "INFO:root:Batch 700, training loss : 0.022540577666729657\n",
      "INFO:root:Batch 800, training loss : 0.01985781945661767\n",
      "INFO:root:Batch 900, training loss : 0.017706382785195873\n",
      "INFO:root:Batch 1000, training loss : 0.01596998397326942\n",
      "INFO:root:Batch 1100, training loss : 0.014542117993635487\n",
      "INFO:root:Batch 1200, training loss : 0.020054153430542173\n",
      "INFO:root:Batch 1300, training loss : 0.025355374496192008\n",
      "INFO:root:Batch 1400, training loss : 0.023544761072301554\n",
      "INFO:root:Batch 1500, training loss : 0.02197555538067693\n",
      "INFO:root:Batch 1600, training loss : 0.020602494142065435\n",
      "INFO:root:Batch 1700, training loss : 0.01939096475141206\n",
      "INFO:root:Batch 1800, training loss : 0.018314040995537954\n",
      "INFO:root:Batch 1900, training loss : 0.017350467282700597\n",
      "INFO:root:Batch 2000, training loss : 0.016483243729028388\n",
      "INFO:root:Batch 2100, training loss : 0.015698609886156102\n",
      "INFO:root:Batch 2200, training loss : 0.014985302220326624\n",
      "INFO:root:Batch 2300, training loss : 0.014334016954348595\n",
      "INFO:root:Epoch 9/10, training loss: 0.013875700526830504 [16.032054901123047s]\n",
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.03717196814144935\n",
      "INFO:root:Dev acc @ top-1: 0.9958784131890778  correct: 1933 out of 1941\n",
      "INFO:root:Saving model to temp_output/BGL_11k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Batch 100, training loss : 1.5852898063712927e-05\n",
      "INFO:root:Batch 200, training loss : 8.30434233705546e-06\n",
      "INFO:root:Batch 300, training loss : 5.656232221250927e-06\n",
      "INFO:root:Batch 400, training loss : 0.00957611117281104\n",
      "INFO:root:Batch 500, training loss : 0.007662648221216955\n",
      "INFO:root:Batch 600, training loss : 0.006386846117001615\n",
      "INFO:root:Batch 700, training loss : 0.02226808499109544\n",
      "INFO:root:Batch 800, training loss : 0.01959973208021616\n",
      "INFO:root:Batch 900, training loss : 0.01746529457677708\n",
      "INFO:root:Batch 1000, training loss : 0.01574471465910898\n",
      "INFO:root:Batch 1100, training loss : 0.014331332110008723\n",
      "INFO:root:Batch 1200, training loss : 0.019432215959636446\n",
      "INFO:root:Batch 1300, training loss : 0.024670268044276897\n",
      "INFO:root:Batch 1400, training loss : 0.022908506242592313\n",
      "INFO:root:Batch 1500, training loss : 0.021381642597158777\n",
      "INFO:root:Batch 1600, training loss : 0.020045632660605897\n",
      "INFO:root:Batch 1700, training loss : 0.018866795665416925\n",
      "INFO:root:Batch 1800, training loss : 0.017818938373086977\n",
      "INFO:root:Batch 1900, training loss : 0.016881376452622977\n",
      "INFO:root:Batch 2000, training loss : 0.01603756965132905\n",
      "INFO:root:Batch 2100, training loss : 0.015274118477073665\n",
      "INFO:root:Batch 2200, training loss : 0.01458006691574468\n",
      "INFO:root:Batch 2300, training loss : 0.01394635864570499\n",
      "INFO:root:Epoch 10/10, training loss: 0.013500413515212247 [16.065457105636597s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating dev data.\n",
      "INFO:root:Dev Loss: 0.033396849023401655\n",
      "INFO:root:Dev acc @ top-1: 0.9948480164863472  correct: 1931 out of 1941\n",
      "INFO:root:Saving model to temp_output/BGL_11k_parsed_session_supervised_AD/model_lstm/model.ckpt\n",
      "INFO:root:Loading model from temp_output/BGL_11k_parsed_session_supervised_AD/model_lstm/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "anomaly_detector = NNAnomalyDetector(config=config.nn_anomaly_detection_config)\n",
    "anomaly_detector.fit(train_features, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating test data.\n",
      "INFO:root:Finish inference. [106.44914507865906s]\n",
      "INFO:root:Calculating acc sum.\n",
      "INFO:root:Finish generating store_df.\n",
      "INFO:root:Finish counting [7.3923351764678955s]\n",
      "INFO:root:Best result: f1: 1.0 rc: 1.0 pc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 1.0, 'rc': 1.0, 'pc': 1.0, 'pred': 0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "1803    1\n",
      "1804    1\n",
      "1805    1\n",
      "1806    1\n",
      "1807    1\n",
      "Name: window_pred_anomaly_8, Length: 1808, dtype: int64, 'true': 0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "1803    1\n",
      "1804    1\n",
      "1805    1\n",
      "1806    1\n",
      "1807    1\n",
      "Name: window_anomalies, Length: 1808, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "predict_results = anomaly_detector.predict(test_features)\n",
    "print (predict_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
