{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Anomaly Detection on HDFS Dataset using LogBERT model\n",
    "This is a running example of the end-to-end workflow of Log Anomaly Detection on public dataset HDFS using the LogBERT model\n",
    "\n",
    "It is very similar to that using the LSTM model. We will only mark out the dataset specific portions of this workflow i.e. the parts that differ between the two models. \n",
    "\n",
    "For a more complete elaboration of the full workflow please refer to `hdfs_lstm_unsupervised_parsed_sequential.ipynb` notebook.\n",
    "\n",
    "We have skipped the parsing step for this workflow since LogBERT model does not need any parsing of the raw logs and directly works on the preprocessed log data (with or without any log partitioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from logai.applications.openset.anomaly_detection.openset_anomaly_detection_workflow import OpenSetADWorkflowConfig, validate_config_dict\n",
    "from logai.utils.file_utils import read_file\n",
    "from logai.utils.dataset_utils import split_train_dev_test_for_anomaly_detection\n",
    "import logging \n",
    "from logai.dataloader.data_loader import FileDataLoader\n",
    "from logai.preprocess.hdfs_preprocessor import HDFSPreprocessor\n",
    "from logai.information_extraction.log_parser import LogParser\n",
    "from logai.preprocess.openset_partitioner import OpenSetPartitioner\n",
    "from logai.analysis.nn_anomaly_detector import NNAnomalyDetector\n",
    "from logai.information_extraction.log_vectorizer import LogVectorizer\n",
    "from logai.utils import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"configs/hdfs_logbert_config.yaml\"\n",
    "config_parsed = read_file(config_path)\n",
    "config_dict = config_parsed[\"workflow_config\"]\n",
    "\n",
    "config = OpenSetADWorkflowConfig.from_dict(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Receiving block blk_-1608999687919862906 src: ...\n",
      "1       BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...\n",
      "2       Receiving block blk_-1608999687919862906 src: ...\n",
      "3       Receiving block blk_-1608999687919862906 src: ...\n",
      "4       PacketResponder 1 for block blk_-1608999687919...\n",
      "                              ...                        \n",
      "4514    Deleting block blk_-2126554733521224025 file /...\n",
      "4515    Deleting block blk_-66330728533676520 file /mn...\n",
      "4516    Deleting block blk_872694497849122755 file /mn...\n",
      "4517    Deleting block blk_3947106522258141922 file /m...\n",
      "4518    Deleting block blk_-774246298521956028 file /m...\n",
      "Name: logline, Length: 4519, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataloader = FileDataLoader(config.data_loader_config)\n",
    "logrecord = dataloader.load_data()\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           Receiving block BLOCK src IP INT dest IP INT \n",
      "1       BLOCK NameSystem.allocateBlock /mnt/hadoop/map...\n",
      "2           Receiving block BLOCK src IP INT dest IP INT \n",
      "3           Receiving block BLOCK src IP INT dest IP INT \n",
      "4         PacketResponder INT for block BLOCK terminating\n",
      "                              ...                        \n",
      "4514    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "4515    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "4516    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "4517    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "4518    Deleting block BLOCK file /mnt/hadoop/dfs/data...\n",
      "Name: logline, Length: 4519, dtype: object\n"
     ]
    }
   ],
   "source": [
    "preprocessor = HDFSPreprocessor(config.preprocessor_config, config.label_filepath)\n",
    "preprocessed_filepath = os.path.join(config.output_dir, 'HDFS_5k_processed.csv')            \n",
    "logrecord = preprocessor.clean_log(logrecord)\n",
    "logrecord.save_to_csv(preprocessed_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Receiving block BLOCK src IP INT dest IP INT [...\n",
      "1      Receiving block BLOCK src IP INT dest IP INT [...\n",
      "2      Receiving block BLOCK src IP INT dest IP INT [...\n",
      "3      Receiving block BLOCK src IP INT dest IP INT [...\n",
      "4      Receiving block BLOCK src IP INT dest IP INT [...\n",
      "                             ...                        \n",
      "105    Receiving block BLOCK src IP INT dest IP INT [...\n",
      "106    Receiving block BLOCK src IP INT dest IP INT [...\n",
      "107    Receiving block BLOCK src IP INT dest IP INT [...\n",
      "108    Receiving block BLOCK src IP INT dest IP INT [...\n",
      "109    Receiving block BLOCK src IP INT dest IP INT [...\n",
      "Name: logline, Length: 110, dtype: object\n"
     ]
    }
   ],
   "source": [
    "partitioner = OpenSetPartitioner(config.open_set_partitioner_config)\n",
    "partitioned_filepath = os.path.join(config.output_dir, 'HDFS_5k_nonparsed_session.csv')\n",
    "logrecord = partitioner.partition(logrecord)\n",
    "logrecord.save_to_csv(partitioned_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_train/dev/test:  75 8 27\n",
      "Train/Dev/Test Anomalous 0 0 10\n",
      "Train/Dev/Test Normal 75 8 17\n"
     ]
    }
   ],
   "source": [
    "train_filepath = os.path.join(config.output_dir, 'HDFS_5k_nonparsed_session_supervised_train.csv')\n",
    "dev_filepath = os.path.join(config.output_dir, 'HDFS_5k_nonparsed_session_supervised_dev.csv')\n",
    "test_filepath = os.path.join(config.output_dir, 'HDFS_5k_nonparsed_session_supervised_test.csv')\n",
    "\n",
    "(train_data, dev_data, test_data) = split_train_dev_test_for_anomaly_detection(\n",
    "                logrecord,training_type=config.training_type,\n",
    "                test_data_frac_neg_class=config.test_data_frac_neg,\n",
    "                test_data_frac_pos_class=config.test_data_frac_pos,\n",
    "                shuffle=config.train_test_shuffle\n",
    "            )\n",
    "\n",
    "train_data.save_to_csv(train_filepath)\n",
    "dev_data.save_to_csv(dev_filepath)\n",
    "test_data.save_to_csv(test_filepath)\n",
    "print ('Train/Dev/Test Anomalous', len(train_data.labels[train_data.labels[constants.LABELS]==1]), \n",
    "                                   len(dev_data.labels[dev_data.labels[constants.LABELS]==1]), \n",
    "                                   len(test_data.labels[test_data.labels[constants.LABELS]==1]))\n",
    "print ('Train/Dev/Test Normal', len(train_data.labels[train_data.labels[constants.LABELS]==0]), \n",
    "                                   len(dev_data.labels[dev_data.labels[constants.LABELS]==0]), \n",
    "                                   len(test_data.labels[test_data.labels[constants.LABELS]==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming log data to vectors using LogVectorizer\n",
    "\n",
    "This step in the code is exactly identical across whichever vectorizer or anomaly detector you choose. All the differences are in the config yaml file where you can specify the algorithm name and algorithm parameters. Each vectorizer algorithm has its own Config or Param dataclass for storing its custom hyperparameters. \n",
    "For the exact parameters of the algorithm of your choice, head to the documentation of the algorithm's config class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0017498017bb49829168b78e62936dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dec6520d5ca4d218bb53a260154160f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872033011f144ab49555c6291d023560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d337c20ae108484d919dd0e13deca5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5708ef612a6342eca96977e84361882e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7024cc4e4ee4332b2e16ee402e01779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c856492fe32f4a1fab59d49bd8febff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f66c8e746424716829171b845b8790c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7697b9d6a43464886c776a483fd93a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13f4322b07448a5895861f62f38b0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f42f4eb6e064002a1801adf1ff16ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6cede4372045368bffb3bd8afe2ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 75\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "vectorizer = LogVectorizer(config.log_vectorizer_config)\n",
    "vectorizer.fit(train_data)\n",
    "train_features = vectorizer.transform(train_data)\n",
    "dev_features = vectorizer.transform(dev_data)\n",
    "test_features = vectorizer.transform(test_data)\n",
    "print (train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection using LogBERT \n",
    "\n",
    "This step in the code is exactly identical across whichever neural anomaly detector (NNAnomalyDetector) you choose. All the differences are in the config yaml file where you can specify the algorithm name and algorithm parameters. Each algorithm has its own Config or Param dataclass for storing its custom hyperparameters. \n",
    "For the exact parameters of the algorithm of your choice, head to the documentation of the algorithm's config class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized data collator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 75\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 190\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [190/190 34:21, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.526500</td>\n",
       "      <td>0.862205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.744600</td>\n",
       "      <td>0.607761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.717900</td>\n",
       "      <td>0.460633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to temp_output/HDFS_5k_parsed_session_supervised_AD/bert-base-cased/checkpoint-50\n",
      "Configuration saved in temp_output/HDFS_5k_parsed_session_supervised_AD/bert-base-cased/checkpoint-50/config.json\n",
      "Model weights saved in temp_output/HDFS_5k_parsed_session_supervised_AD/bert-base-cased/checkpoint-50/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to temp_output/HDFS_5k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100\n",
      "Configuration saved in temp_output/HDFS_5k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100/config.json\n",
      "Model weights saved in temp_output/HDFS_5k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to temp_output/HDFS_5k_parsed_session_supervised_AD/bert-base-cased/checkpoint-150\n",
      "Configuration saved in temp_output/HDFS_5k_parsed_session_supervised_AD/bert-base-cased/checkpoint-150/config.json\n",
      "Model weights saved in temp_output/HDFS_5k_parsed_session_supervised_AD/bert-base-cased/checkpoint-150/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anomaly_detector = NNAnomalyDetector(config=config.nn_anomaly_detection_config)\n",
    "anomaly_detector.fit(train_features, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model from /Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/temp_output/HDFS_5k_parsed_session_supervised_AD/bert-base-cased/checkpoint-150\n",
      "loading configuration file /Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/temp_output/HDFS_5k_parsed_session_supervised_AD/bert-base-cased/checkpoint-150/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 312\n",
      "}\n",
      "\n",
      "loading weights file /Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/temp_output/HDFS_5k_parsed_session_supervised_AD/bert-base-cased/checkpoint-150/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at /Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/temp_output/HDFS_5k_parsed_session_supervised_AD/bert-base-cased/checkpoint-150.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4792c5dbb016483bad13b53765a92892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 105\n",
      "  Batch size = 256\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 1.014678955078125 test_runtime: 83.2527 test_samples/s: 1.261\n",
      "INFO:root:number of original test instances 27\n",
      "INFO:root:loss_mean Pos scores:  mean: 2.059218628594652, std: 1.7926128562940402\n",
      "INFO:root:loss_mean Neg scores: mean: 0.5299380107454079, std: 0.39768380503463113\n",
      "INFO:root:AUC of loss_mean: 0.7235294117647059\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_max Pos scores:  mean: 5.7560831069946286, std: 3.14190306540753\n",
      "INFO:root:loss_max Neg scores: mean: 2.943949373329387, std: 1.7687664243041483\n",
      "INFO:root:AUC of loss_max: 0.7235294117647059\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_top6_mean Pos scores:  mean: 2.400115772363885, std: 1.992208159608372\n",
      "INFO:root:loss_top6_mean Neg scores: mean: 0.6830175074237381, std: 0.5114021108113075\n",
      "INFO:root:AUC of loss_top6_mean: 0.7235294117647059\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_prob Pos scores:  mean: 0.21630861336986226, std: 0.05769911577260041\n",
      "INFO:root:scores_top6_max_prob Neg scores: mean: 0.18187326222073796, std: 0.014334467801007033\n",
      "INFO:root:AUC of scores_top6_max_prob: 0.7647058823529411\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_min_logprob Pos scores:  mean: 0.26993126423330976, std: 0.07807216640992373\n",
      "INFO:root:scores_top6_min_logprob Neg scores: mean: 0.22321251803077757, std: 0.021484902073234186\n",
      "INFO:root:AUC of scores_top6_min_logprob: 0.7705882352941176\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_entropy Pos scores:  mean: 0.8942615666737159, std: 0.22807753721388022\n",
      "INFO:root:scores_top6_max_entropy Neg scores: mean: 0.7335651825711716, std: 0.04560924868055203\n",
      "INFO:root:AUC of scores_top6_max_entropy: 0.8\n",
      "INFO:root:\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 104\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 0.9861030578613281 test_runtime: 82.683 test_samples/s: 1.258\n",
      "INFO:root:number of original test instances 27\n",
      "***** Running Prediction *****\n",
      "  Num examples = 104\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 1.1745320558547974 test_runtime: 88.791 test_samples/s: 1.171\n",
      "INFO:root:number of original test instances 27\n",
      "INFO:root:loss_mean Pos scores:  mean: 2.1219184682465544, std: 1.6983737911106986\n",
      "INFO:root:loss_mean Neg scores: mean: 0.5928176043682969, std: 0.2924837395173191\n",
      "INFO:root:AUC of loss_mean: 0.7470588235294118\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_max Pos scores:  mean: 6.691045713424683, std: 2.9804914308932755\n",
      "INFO:root:loss_max Neg scores: mean: 4.090087729341843, std: 1.3805904426704225\n",
      "INFO:root:AUC of loss_max: 0.6882352941176471\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_top6_mean Pos scores:  mean: 3.4820845513397614, std: 2.2425742149074077\n",
      "INFO:root:loss_top6_mean Neg scores: mean: 1.3439611899209956, std: 0.7046433189823426\n",
      "INFO:root:AUC of loss_top6_mean: 0.7764705882352941\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_prob Pos scores:  mean: 0.31607609084910815, std: 0.028842391046279624\n",
      "INFO:root:scores_top6_max_prob Neg scores: mean: 0.2965105602067281, std: 0.004912850334116107\n",
      "INFO:root:AUC of scores_top6_max_prob: 0.8352941176470589\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_min_logprob Pos scores:  mean: 0.40452349997229053, std: 0.04239718420822502\n",
      "INFO:root:scores_top6_min_logprob Neg scores: mean: 0.37303297463013163, std: 0.006056889125125173\n",
      "INFO:root:AUC of scores_top6_min_logprob: 0.8352941176470589\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_entropy Pos scores:  mean: 1.1932347184254062, std: 0.1418354789028915\n",
      "INFO:root:scores_top6_max_entropy Neg scores: mean: 1.0630393121513275, std: 0.024435813863690863\n",
      "INFO:root:AUC of scores_top6_max_entropy: 0.8117647058823529\n",
      "INFO:root:\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 104\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 1.1727038621902466 test_runtime: 93.7178 test_samples/s: 1.11\n",
      "INFO:root:number of original test instances 27\n",
      "***** Running Prediction *****\n",
      "  Num examples = 104\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 1.1584101915359497 test_runtime: 86.6201 test_samples/s: 1.201\n",
      "INFO:root:number of original test instances 27\n",
      "INFO:root:loss_mean Pos scores:  mean: 2.2554494030398713, std: 1.8150099338811976\n",
      "INFO:root:loss_mean Neg scores: mean: 0.609534556013258, std: 0.32165884581679327\n",
      "INFO:root:AUC of loss_mean: 0.8352941176470587\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_max Pos scores:  mean: 7.351688361167907, std: 2.7813355088245744\n",
      "INFO:root:loss_max Neg scores: mean: 4.5056345182306625, std: 1.5344308862051492\n",
      "INFO:root:AUC of loss_max: 0.7441176470588234\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_top6_mean Pos scores:  mean: 4.887380647312642, std: 2.8028220232620416\n",
      "INFO:root:loss_top6_mean Neg scores: mean: 1.8708893243418216, std: 1.0048697887345033\n",
      "INFO:root:AUC of loss_top6_mean: 0.8352941176470587\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_prob Pos scores:  mean: 0.326788921530048, std: 0.03757485806268242\n",
      "INFO:root:scores_top6_max_prob Neg scores: mean: 0.32176246679101894, std: 0.015138238006540416\n",
      "INFO:root:AUC of scores_top6_max_prob: 0.5764705882352941\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_min_logprob Pos scores:  mean: 0.431262838980183, std: 0.05186462793363405\n",
      "INFO:root:scores_top6_min_logprob Neg scores: mean: 0.41335328075779126, std: 0.02046371460474274\n",
      "INFO:root:AUC of scores_top6_min_logprob: 0.5764705882352941\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_entropy Pos scores:  mean: 1.2477047782805228, std: 0.14859352680436985\n",
      "INFO:root:scores_top6_max_entropy Neg scores: mean: 1.1496512033045294, std: 0.025594207126439575\n",
      "INFO:root:AUC of scores_top6_max_entropy: 0.7352941176470589\n",
      "INFO:root:\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 104\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 0.9672116637229919 test_runtime: 83.5388 test_samples/s: 1.245\n",
      "INFO:root:number of original test instances 27\n",
      "***** Running Prediction *****\n",
      "  Num examples = 104\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 0.969925582408905 test_runtime: 84.7546 test_samples/s: 1.227\n",
      "INFO:root:number of original test instances 27\n",
      "INFO:root:loss_mean Pos scores:  mean: 2.349126224921584, std: 2.1523467967824894\n",
      "INFO:root:loss_mean Neg scores: mean: 0.53474554428909, std: 0.28215333184122304\n",
      "INFO:root:AUC of loss_mean: 0.8294117647058823\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_max Pos scores:  mean: 7.653564143180847, std: 3.0611919169082427\n",
      "INFO:root:loss_max Neg scores: mean: 4.604733425028184, std: 1.4066104234214887\n",
      "INFO:root:AUC of loss_max: 0.7441176470588234\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_top6_mean Pos scores:  mean: 5.196850432952244, std: 2.9924530904184525\n",
      "INFO:root:loss_top6_mean Neg scores: mean: 2.0070443147263837, std: 1.023917104014254\n",
      "INFO:root:AUC of loss_top6_mean: 0.8294117647058823\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_prob Pos scores:  mean: 0.33317578554981286, std: 0.04539746382246543\n",
      "INFO:root:scores_top6_max_prob Neg scores: mean: 0.3474506453117904, std: 0.015463721962154215\n",
      "INFO:root:AUC of scores_top6_max_prob: 0.38823529411764707\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_min_logprob Pos scores:  mean: 0.4456323600736344, std: 0.05971918188205437\n",
      "INFO:root:scores_top6_min_logprob Neg scores: mean: 0.4565874318214852, std: 0.01947039185688739\n",
      "INFO:root:AUC of scores_top6_min_logprob: 0.45882352941176474\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_entropy Pos scores:  mean: 1.2313360758953624, std: 0.10373346316549091\n",
      "INFO:root:scores_top6_max_entropy Neg scores: mean: 1.2002811115600316, std: 0.025821831551886405\n",
      "INFO:root:AUC of scores_top6_max_entropy: 0.4970588235294117\n",
      "INFO:root:\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 104\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 1.01661217212677 test_runtime: 83.5752 test_samples/s: 1.244\n",
      "INFO:root:number of original test instances 27\n",
      "***** Running Prediction *****\n",
      "  Num examples = 104\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 1.0347822904586792 test_runtime: 84.5661 test_samples/s: 1.23\n",
      "INFO:root:number of original test instances 27\n",
      "INFO:root:loss_mean Pos scores:  mean: 2.4548787350300554, std: 2.3441222706009706\n",
      "INFO:root:loss_mean Neg scores: mean: 0.4871146835758065, std: 0.214966075456876\n",
      "INFO:root:AUC of loss_mean: 0.7999999999999999\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_max Pos scores:  mean: 7.775362467765808, std: 3.1761796382891503\n",
      "INFO:root:loss_max Neg scores: mean: 4.826125748017255, std: 1.4327840530789102\n",
      "INFO:root:AUC of loss_max: 0.7441176470588234\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_top6_mean Pos scores:  mean: 5.497082110235675, std: 3.2717929341947243\n",
      "INFO:root:loss_top6_mean Neg scores: mean: 2.172644008081893, std: 0.9551029668480685\n",
      "INFO:root:AUC of loss_top6_mean: 0.7941176470588235\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_prob Pos scores:  mean: 0.33998988034824523, std: 0.05070171762118567\n",
      "INFO:root:scores_top6_max_prob Neg scores: mean: 0.3777509571047, std: 0.007681180353872846\n",
      "INFO:root:AUC of scores_top6_max_prob: 0.2411764705882353\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_min_logprob Pos scores:  mean: 0.46017570178438383, std: 0.05999816117576101\n",
      "INFO:root:scores_top6_min_logprob Neg scores: mean: 0.5025932865454724, std: 0.011150802774502904\n",
      "INFO:root:AUC of scores_top6_min_logprob: 0.2647058823529412\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_entropy Pos scores:  mean: 1.213651625315348, std: 0.0827533749895994\n",
      "INFO:root:scores_top6_max_entropy Neg scores: mean: 1.20644193135348, std: 0.022903260211833948\n",
      "INFO:root:AUC of scores_top6_max_entropy: 0.5029411764705882\n",
      "INFO:root:\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 104\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 0.9827648997306824 test_runtime: 82.3832 test_samples/s: 1.262\n",
      "INFO:root:number of original test instances 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     indices   max_loss   sum_loss num_loss  \\\n",
      "0          0   0.078862   0.400576        8   \n",
      "1          0  10.861187   78.08194        8   \n",
      "2          1   8.379167   43.85025        9   \n",
      "3          1   0.332565   1.821065        8   \n",
      "4          1   8.497158  52.730793        8   \n",
      "...      ...        ...        ...      ...   \n",
      "1036      25   0.028344   0.152666        8   \n",
      "1037      26    6.27078  32.057602        8   \n",
      "1038      26   5.805102  25.056643        8   \n",
      "1039      26   1.282481   5.625502        8   \n",
      "1040      26   0.028524    0.15167        8   \n",
      "\n",
      "                                              top6_loss  \\\n",
      "0     [0.0788615420460701, 0.07201781123876572, 0.05...   \n",
      "1     [10.861186981201172, 10.619929313659668, 10.04...   \n",
      "2     [8.379166603088379, 7.949586391448975, 7.41764...   \n",
      "3     [0.33256518840789795, 0.2971160411834717, 0.25...   \n",
      "4     [8.49715805053711, 8.332608222961426, 7.788794...   \n",
      "...                                                 ...   \n",
      "1036  [0.028343606740236282, 0.026070745661854744, 0...   \n",
      "1037  [6.270779609680176, 5.324167251586914, 5.31461...   \n",
      "1038  [5.805102348327637, 5.461217880249023, 5.18682...   \n",
      "1039  [1.2824805974960327, 1.1324663162231445, 1.061...   \n",
      "1040  [0.028524011373519897, 0.025363657623529434, 0...   \n",
      "\n",
      "                                          top6_max_prob  \\\n",
      "0     [0.9241678714752197, 0.9305143356323242, 0.947...   \n",
      "1     [0.41589871048927307, 0.44219473004341125, 0.6...   \n",
      "2     [0.4242318868637085, 0.5208358764648438, 0.551...   \n",
      "3     [0.7170819044113159, 0.7429578304290771, 0.773...   \n",
      "4     [0.3752920627593994, 0.5671303272247314, 0.651...   \n",
      "...                                                 ...   \n",
      "1036  [0.9720543026924133, 0.9742661714553833, 0.975...   \n",
      "1037  [0.4844566881656647, 0.5114573240280151, 0.877...   \n",
      "1038  [0.926629900932312, 0.9360955953598022, 0.9530...   \n",
      "1039  [0.44495025277137756, 0.5472314357757568, 0.57...   \n",
      "1040  [0.9718789458274841, 0.9749553203582764, 0.975...   \n",
      "\n",
      "                                       top6_min_logprob  \\\n",
      "0     [0.0788615420460701, 0.07201781123876572, 0.05...   \n",
      "1     [0.8773135542869568, 0.8160049319267273, 0.418...   \n",
      "2     [0.8574750423431396, 0.6523203253746033, 0.595...   \n",
      "3     [0.33256518840789795, 0.2971160411834717, 0.25...   \n",
      "4     [0.9800506830215454, 0.5671662092208862, 0.428...   \n",
      "...                                                 ...   \n",
      "1036  [0.028343606740236282, 0.026070745661854744, 0...   \n",
      "1037  [0.724727213382721, 0.670491099357605, 0.13073...   \n",
      "1038  [0.07620105147361755, 0.06603769958019257, 0.0...   \n",
      "1039  [0.8097928166389465, 0.6028834581375122, 0.546...   \n",
      "1040  [0.028524011373519897, 0.025363657623529434, 0...   \n",
      "\n",
      "                                       top6_max_entropy  \n",
      "0     [0.5729002952575684, 0.476054847240448, 0.4086...  \n",
      "1     [1.8378887176513672, 1.7860469818115234, 1.677...  \n",
      "2     [2.1699061393737793, 1.8110240697860718, 1.799...  \n",
      "3     [1.27303147315979, 1.2209243774414062, 1.10064...  \n",
      "4     [1.8748129606246948, 1.3444006443023682, 1.160...  \n",
      "...                                                 ...  \n",
      "1036  [0.2693485617637634, 0.2324058711528778, 0.222...  \n",
      "1037  [2.395415782928467, 1.4185678958892822, 0.7393...  \n",
      "1038  [0.509660542011261, 0.4311492443084717, 0.3366...  \n",
      "1039  [1.468814730644226, 1.1594902276992798, 1.1191...  \n",
      "1040  [0.2711292803287506, 0.22759972512722015, 0.22...  \n",
      "\n",
      "[1041 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "predict_results = anomaly_detector.predict(test_features)\n",
    "print (predict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
