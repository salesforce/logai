{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from logai.applications.openset.anomaly_detection.openset_anomaly_detection_workflow import OpenSetADWorkflowConfig, validate_config_dict\n",
    "from logai.utils.file_utils import read_file\n",
    "from logai.utils.dataset_utils import split_train_dev_test_for_anomaly_detection\n",
    "import logging \n",
    "from logai.dataloader.data_loader import FileDataLoader\n",
    "from logai.preprocess.bgl_preprocessor import BGLPreprocessor\n",
    "from logai.information_extraction.log_parser import LogParser\n",
    "from logai.preprocess.openset_partitioner import OpenSetPartitioner\n",
    "from logai.analysis.nn_anomaly_detector import NNAnomalyDetector\n",
    "from logai.information_extraction.log_vectorizer import LogVectorizer\n",
    "from logai.utils import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"bgl_logbert_config.yaml\"\n",
    "config_parsed = read_file(config_path)\n",
    "config_dict = config_parsed[\"workflow_config\"]\n",
    "validate_config_dict(config_dict)\n",
    "config = OpenSetADWorkflowConfig.from_dict(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         RAS KERNEL INFO instruction cache parity error...\n",
      "1         RAS KERNEL INFO instruction cache parity error...\n",
      "2         RAS KERNEL INFO instruction cache parity error...\n",
      "3         RAS KERNEL INFO instruction cache parity error...\n",
      "4         RAS KERNEL INFO instruction cache parity error...\n",
      "                                ...                        \n",
      "358455    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358456    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358457    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358458    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358459    RAS KERNEL FATAL idoproxy communication failur...\n",
      "Name: logline, Length: 358460, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/logai/dataloader/data_loader.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected[constants.LOG_TIMESTAMPS] = pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "dataloader = FileDataLoader(config.data_loader_config)\n",
    "logrecord = dataloader.load_data()\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         RAS KERNEL INFO instruction cache parity error...\n",
      "1         RAS KERNEL INFO instruction cache parity error...\n",
      "2         RAS KERNEL INFO instruction cache parity error...\n",
      "3         RAS KERNEL INFO instruction cache parity error...\n",
      "4         RAS KERNEL INFO instruction cache parity error...\n",
      "                                ...                        \n",
      "358455    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358456    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358457    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358458    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358459    RAS KERNEL FATAL idoproxy communication failur...\n",
      "Name: logline, Length: 358460, dtype: object\n"
     ]
    }
   ],
   "source": [
    "preprocessor = BGLPreprocessor(config.preprocessor_config)\n",
    "preprocessed_filepath = os.path.join(config.output_dir, 'BGL_11k_processed.csv')            \n",
    "logrecord = preprocessor.clean_log(logrecord)\n",
    "logrecord.save_to_csv(preprocessed_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       RAS KERNEL INFO instruction cache parity error...\n",
      "1       RAS KERNEL INFO instruction cache parity error...\n",
      "2       RAS KERNEL INFO instruction cache parity error...\n",
      "3       RAS KERNEL INFO instruction cache parity error...\n",
      "4       RAS KERNEL INFO instruction cache parity error...\n",
      "                              ...                        \n",
      "1848    RAS APP FATAL ciod Error reading message prefi...\n",
      "1849    RAS KERNEL FATAL Lustre mount FAILED ALPHANUM ...\n",
      "1850    RAS KERNEL FATAL Lustre mount FAILED ALPHANUM ...\n",
      "1851    RAS KERNEL FATAL Lustre mount FAILED ALPHANUM ...\n",
      "1852    RAS KERNEL FATAL idoproxy communication failur...\n",
      "Name: logline, Length: 1853, dtype: object\n"
     ]
    }
   ],
   "source": [
    "partitioner = OpenSetPartitioner(config.open_set_partitioner_config)\n",
    "partitioned_filepath = os.path.join(config.output_dir, 'BGL_11k_nonparsed_session.csv')\n",
    "logrecord = partitioner.partition(logrecord)\n",
    "logrecord.save_to_csv(partitioned_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_train/dev/test:  40 5 1808\n",
      "Train/Dev/Test Anomalous 0 0 1808\n",
      "Train/Dev/Test Normal 40 5 0\n"
     ]
    }
   ],
   "source": [
    "train_filepath = os.path.join(config.output_dir, 'BGL_11k_nonparsed_session_supervised_train.csv')\n",
    "dev_filepath = os.path.join(config.output_dir, 'BGL_11k_nonparsed_session_supervised_dev.csv')\n",
    "test_filepath = os.path.join(config.output_dir, 'BGL_11k_nonparsed_session_supervised_test.csv')\n",
    "\n",
    "(train_data, dev_data, test_data) = split_train_dev_test_for_anomaly_detection(\n",
    "                logrecord,training_type=config.training_type,\n",
    "                test_data_frac_neg_class=config.test_data_frac_neg,\n",
    "                test_data_frac_pos_class=config.test_data_frac_pos,\n",
    "                shuffle=config.train_test_shuffle\n",
    "            )\n",
    "\n",
    "train_data.save_to_csv(train_filepath)\n",
    "dev_data.save_to_csv(dev_filepath)\n",
    "test_data.save_to_csv(test_filepath)\n",
    "print ('Train/Dev/Test Anomalous', len(train_data.labels[train_data.labels[constants.LABELS]==1]), \n",
    "                                   len(dev_data.labels[dev_data.labels[constants.LABELS]==1]), \n",
    "                                   len(test_data.labels[test_data.labels[constants.LABELS]==1]))\n",
    "print ('Train/Dev/Test Normal', len(train_data.labels[train_data.labels[constants.LABELS]==0]), \n",
    "                                   len(dev_data.labels[dev_data.labels[constants.LABELS]==0]), \n",
    "                                   len(test_data.labels[test_data.labels[constants.LABELS]==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c3ec58f07b43e494607b7c22dc0af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a1e05ef9de4965956bb57640ba1ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2dab9c10de249ababcf6c60d0ae0018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7fdc2b453b4014bad405416a31b53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994e12198b9d4b4697b21118e0309de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bf2faa2e5b4670a4375a72e98f691a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9115a287f50947cb85a0bd2d2429f4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f2e41280224f618979e9c5874b8962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819a7ccf063442baa0c32394a57cd693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fd6277b1494e75b38fcfb577d35df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590fb1d29ea140bab59af4d7c7ec5525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e76bc7c7e34372834301e27f07eba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 40\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "vectorizer = LogVectorizer(config.log_vectorizer_config)\n",
    "vectorizer.fit(train_data)\n",
    "train_features = vectorizer.transform(train_data)\n",
    "dev_features = vectorizer.transform(dev_data)\n",
    "test_features = vectorizer.transform(test_data)\n",
    "print (train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized data collator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 40\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 15:53, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.082700</td>\n",
       "      <td>3.418151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.826200</td>\n",
       "      <td>3.554583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-50\n",
      "Configuration saved in temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-50/config.json\n",
      "Model weights saved in temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-50/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100\n",
      "Configuration saved in temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100/config.json\n",
      "Model weights saved in temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anomaly_detector = NNAnomalyDetector(config=config.nn_anomaly_detection_config)\n",
    "anomaly_detector.fit(train_features, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results = anomaly_detector.predict(test_features)\n",
    "print (predict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
