{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Anomaly Detection on BGL dataset using LogBERT model\n",
    "This is a running example of an end-to-end workflow of Log Anomaly Detection on public dataset HDFS using the LogBERT model.\n",
    "\n",
    "There are similar workflows on the BGL datasets using other anomaly detectors (like LSTM based one in `bgl_lstm_unsupervised_parsed_sequential.ipynb`). \n",
    "\n",
    "The actual workflow script is exactly identical in these cases, except in the LogBERT case we choose to skip the log-parsing step. This is simply done following past literature, but there are no restrictions from the LogAI library side. \n",
    "\n",
    "\n",
    "Also check out the other config files that in this directory that cater to other datasets (HDFS), or other experimental configs like (parsing/nonparsing based, sliding/session window based log partitioning, sequential/semantic log feature representations, supervised/unsupervised setting, LSTM/CNN/Transformer/BERT model). \n",
    "\n",
    "To use these different experimental configs, you only need to point to the correct config file and the same workflow code should work perfectly for those!\n",
    "\n",
    "Only in case of changing the dataset (eg. from BGL to HDFS) you need to not only change the config.yaml file but also use the HDFSPreprocessor in the preprocessing step. Note that each custom dataset that are added should have its own Preprocessor class (which should inherit from logai.preproces.preprocessor.Preprocessor). \n",
    "\n",
    "For more complete explanations of each step of the workflow check out the `hdfs_lstm_unsupervised_parsed_sequential.ipynb` notebook instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from logai.applications.openset.anomaly_detection.openset_anomaly_detection_workflow import OpenSetADWorkflowConfig, validate_config_dict\n",
    "from logai.utils.file_utils import read_file\n",
    "from logai.utils.dataset_utils import split_train_dev_test_for_anomaly_detection\n",
    "import logging \n",
    "from logai.dataloader.data_loader import FileDataLoader\n",
    "from logai.preprocess.bgl_preprocessor import BGLPreprocessor\n",
    "from logai.information_extraction.log_parser import LogParser\n",
    "from logai.preprocess.openset_partitioner import OpenSetPartitioner\n",
    "from logai.analysis.nn_anomaly_detector import NNAnomalyDetector\n",
    "from logai.information_extraction.log_vectorizer import LogVectorizer\n",
    "from logai.utils import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"configs/bgl_logbert_config.yaml\"\n",
    "config_parsed = read_file(config_path)\n",
    "config_dict = config_parsed[\"workflow_config\"]\n",
    "config = OpenSetADWorkflowConfig.from_dict(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         RAS KERNEL INFO instruction cache parity error...\n",
      "1         RAS KERNEL INFO instruction cache parity error...\n",
      "2         RAS KERNEL INFO instruction cache parity error...\n",
      "3         RAS KERNEL INFO instruction cache parity error...\n",
      "4         RAS KERNEL INFO instruction cache parity error...\n",
      "                                ...                        \n",
      "358455    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358456    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358457    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358458    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358459    RAS KERNEL FATAL idoproxy communication failur...\n",
      "Name: logline, Length: 358460, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/logai/dataloader/data_loader.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected[constants.LOG_TIMESTAMPS] = pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "dataloader = FileDataLoader(config.data_loader_config)\n",
    "logrecord = dataloader.load_data()\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         RAS KERNEL INFO instruction cache parity error...\n",
      "1         RAS KERNEL INFO instruction cache parity error...\n",
      "2         RAS KERNEL INFO instruction cache parity error...\n",
      "3         RAS KERNEL INFO instruction cache parity error...\n",
      "4         RAS KERNEL INFO instruction cache parity error...\n",
      "                                ...                        \n",
      "358455    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358456    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358457    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358458    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358459    RAS KERNEL FATAL idoproxy communication failur...\n",
      "Name: logline, Length: 358460, dtype: object\n"
     ]
    }
   ],
   "source": [
    "preprocessor = BGLPreprocessor(config.preprocessor_config)\n",
    "preprocessed_filepath = os.path.join(config.output_dir, 'BGL_11k_processed.csv')            \n",
    "logrecord = preprocessor.clean_log(logrecord)\n",
    "logrecord.save_to_csv(preprocessed_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       RAS KERNEL INFO instruction cache parity error...\n",
      "1       RAS KERNEL INFO instruction cache parity error...\n",
      "2       RAS KERNEL INFO instruction cache parity error...\n",
      "3       RAS KERNEL INFO instruction cache parity error...\n",
      "4       RAS KERNEL INFO instruction cache parity error...\n",
      "                              ...                        \n",
      "1848    RAS APP FATAL ciod Error reading message prefi...\n",
      "1849    RAS KERNEL FATAL Lustre mount FAILED ALPHANUM ...\n",
      "1850    RAS KERNEL FATAL Lustre mount FAILED ALPHANUM ...\n",
      "1851    RAS KERNEL FATAL Lustre mount FAILED ALPHANUM ...\n",
      "1852    RAS KERNEL FATAL idoproxy communication failur...\n",
      "Name: logline, Length: 1853, dtype: object\n"
     ]
    }
   ],
   "source": [
    "partitioner = OpenSetPartitioner(config.open_set_partitioner_config)\n",
    "partitioned_filepath = os.path.join(config.output_dir, 'BGL_11k_nonparsed_session.csv')\n",
    "logrecord = partitioner.partition(logrecord)\n",
    "logrecord.save_to_csv(partitioned_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_train/dev/test:  40 5 1808\n",
      "Train/Dev/Test Anomalous 0 0 1808\n",
      "Train/Dev/Test Normal 40 5 0\n"
     ]
    }
   ],
   "source": [
    "train_filepath = os.path.join(config.output_dir, 'BGL_11k_nonparsed_session_unsupervised_train.csv')\n",
    "dev_filepath = os.path.join(config.output_dir, 'BGL_11k_nonparsed_session_unsupervised_dev.csv')\n",
    "test_filepath = os.path.join(config.output_dir, 'BGL_11k_nonparsed_session_unsupervised_test.csv')\n",
    "\n",
    "(train_data, dev_data, test_data) = split_train_dev_test_for_anomaly_detection(\n",
    "                logrecord,training_type=config.training_type,\n",
    "                test_data_frac_neg_class=config.test_data_frac_neg,\n",
    "                test_data_frac_pos_class=config.test_data_frac_pos,\n",
    "                shuffle=config.train_test_shuffle\n",
    "            )\n",
    "\n",
    "train_data.save_to_csv(train_filepath)\n",
    "dev_data.save_to_csv(dev_filepath)\n",
    "test_data.save_to_csv(test_filepath)\n",
    "print ('Train/Dev/Test Anomalous', len(train_data.labels[train_data.labels[constants.LABELS]==1]), \n",
    "                                   len(dev_data.labels[dev_data.labels[constants.LABELS]==1]), \n",
    "                                   len(test_data.labels[test_data.labels[constants.LABELS]==1]))\n",
    "print ('Train/Dev/Test Normal', len(train_data.labels[train_data.labels[constants.LABELS]==0]), \n",
    "                                   len(dev_data.labels[dev_data.labels[constants.LABELS]==0]), \n",
    "                                   len(test_data.labels[test_data.labels[constants.LABELS]==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6d28105e6240c9bfa1fb4685250876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46468938d91340eb8e358560ee56e4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06671dce344b4d6eb0ba9622018fd977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c72a5c7eae4347ae09345bb294e561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef08c14d82664cea9286f589e5456d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b2eff6f09d4cb0b9a55452f5cb060f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c01779fc9e47519fb8d49988736206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541eaaa1093f410e8e8476108127793c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a562b1628934aaa86d2a1e5dca11d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a3aba2ca63404e950a4eef3432e277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac55600d594487cbcf37a9e700a350b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b569d10f074b2c96effbce8c7233dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 40\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "vectorizer = LogVectorizer(config.log_vectorizer_config)\n",
    "vectorizer.fit(train_data)\n",
    "train_features = vectorizer.transform(train_data)\n",
    "dev_features = vectorizer.transform(dev_data)\n",
    "test_features = vectorizer.transform(test_data)\n",
    "print (train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized data collator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 40\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:15, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.407800</td>\n",
       "      <td>7.452397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.909800</td>\n",
       "      <td>3.440429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-50\n",
      "Configuration saved in temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-50/config.json\n",
      "Model weights saved in temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-50/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100\n",
      "Configuration saved in temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100/config.json\n",
      "Model weights saved in temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anomaly_detector = NNAnomalyDetector(config=config.nn_anomaly_detection_config)\n",
    "anomaly_detector.fit(train_features, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model from /Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/examples/jupyter_notebook/nn_ad_benchmarking/temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100\n",
      "loading configuration file /Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/examples/jupyter_notebook/nn_ad_benchmarking/temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 332\n",
      "}\n",
      "\n",
      "loading weights file /Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/examples/jupyter_notebook/nn_ad_benchmarking/temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at /Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/examples/jupyter_notebook/nn_ad_benchmarking/temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c744af9b70d42908ac8065ceadd75e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1808 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1849\n",
      "  Batch size = 256\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 6.922546863555908 test_runtime: 390.1582 test_samples/s: 4.739\n",
      "INFO:root:number of original test instances 1471\n",
      "INFO:root:loss_mean Pos scores:  mean: 6.942281161443237, std: 0.774283953001779\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_max Pos scores:  mean: 8.484454650606613, std: 0.3959560440390038\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_top6_mean Pos scores:  mean: 7.668759094596368, std: 0.5011946579908295\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_prob Pos scores:  mean: 0.8799232241718502, std: 0.0035729415337627285\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_min_logprob Pos scores:  mean: 2.120101489503179, std: 0.030195267421847985\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_entropy Pos scores:  mean: 3.835382603974994, std: 0.04770405087132924\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1849\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 6.888496398925781 test_runtime: 403.7301 test_samples/s: 4.58\n",
      "INFO:root:number of original test instances 1529\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1849\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 6.926351547241211 test_runtime: 382.4757 test_samples/s: 4.834\n",
      "INFO:root:number of original test instances 1582\n",
      "INFO:root:loss_mean Pos scores:  mean: 6.927276679381086, std: 0.3974681718360548\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_max Pos scores:  mean: 8.712498862583628, std: 0.3175124675373986\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_top6_mean Pos scores:  mean: 7.677022547374567, std: 0.2664408461490415\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_prob Pos scores:  mean: 0.8797571338317824, std: 0.003480296778454621\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_min_logprob Pos scores:  mean: 2.1186989939628083, std: 0.02940394305167903\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_entropy Pos scores:  mean: 3.8381765023060175, std: 0.04717771638225031\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1848\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 6.884964466094971 test_runtime: 390.1089 test_samples/s: 4.737\n",
      "INFO:root:number of original test instances 1625\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1848\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 6.922284126281738 test_runtime: 389.3319 test_samples/s: 4.747\n",
      "INFO:root:number of original test instances 1684\n",
      "INFO:root:loss_mean Pos scores:  mean: 6.926799823515818, std: 0.3536372918444847\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_max Pos scores:  mean: 8.763144832608818, std: 0.29607924824482706\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_top6_mean Pos scores:  mean: 7.720762070491575, std: 0.24674667673100265\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_prob Pos scores:  mean: 0.8796520814086478, std: 0.0034284196862021652\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_min_logprob Pos scores:  mean: 2.1178135655845933, std: 0.028972703911315727\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_entropy Pos scores:  mean: 3.840814814081636, std: 0.04634714188861548\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1848\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 6.8713178634643555 test_runtime: 398.9951 test_samples/s: 4.632\n",
      "INFO:root:number of original test instances 1718\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1848\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 6.895295143127441 test_runtime: 389.3883 test_samples/s: 4.746\n",
      "INFO:root:number of original test instances 1757\n",
      "INFO:root:loss_mean Pos scores:  mean: 6.9251040041738445, std: 0.3305131315468509\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_max Pos scores:  mean: 8.789467540463061, std: 0.2957725925801199\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_top6_mean Pos scores:  mean: 7.770443531976949, std: 0.2481919072759722\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_prob Pos scores:  mean: 0.87962998784062, std: 0.0034448253184315857\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_min_logprob Pos scores:  mean: 2.1176362749635103, std: 0.029135540154287348\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_entropy Pos scores:  mean: 3.842922667716204, std: 0.04503843512817689\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1848\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 6.904280662536621 test_runtime: 382.582 test_samples/s: 4.83\n",
      "INFO:root:number of original test instances 1780\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1848\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 6.91431999206543 test_runtime: 383.5132 test_samples/s: 4.819\n",
      "INFO:root:number of original test instances 1805\n",
      "INFO:root:loss_mean Pos scores:  mean: 6.931001467315855, std: 0.3142642478823886\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_max Pos scores:  mean: 8.80874226641457, std: 0.29194871734675143\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_top6_mean Pos scores:  mean: 7.755777619240943, std: 0.26667536160998695\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_prob Pos scores:  mean: 0.8796269210858317, std: 0.0034756403549123503\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_min_logprob Pos scores:  mean: 2.117619333810974, std: 0.029416584341583667\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_entropy Pos scores:  mean: 3.844573212302109, std: 0.04428138132420088\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1848\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 6.865622043609619 test_runtime: 393.0448 test_samples/s: 4.702\n",
      "INFO:root:number of original test instances 1808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      indices  max_loss   sum_loss num_loss  \\\n",
      "0           0  8.259521  57.445042        9   \n",
      "1           0  8.235804  47.465973        8   \n",
      "2           1  8.030133  62.417809        8   \n",
      "3           2  8.638699  68.155144        9   \n",
      "4           2  8.026514  62.823498        8   \n",
      "...       ...       ...        ...      ...   \n",
      "18478    1804  8.474268  63.616337        8   \n",
      "18479    1805   8.47681  52.004642        8   \n",
      "18480    1806   8.77824  55.250885        9   \n",
      "18481    1807  8.028709  56.912701        9   \n",
      "18482    1807  8.787713  63.224758        8   \n",
      "\n",
      "                                               top6_loss  \\\n",
      "0      [8.259520530700684, 8.195176124572754, 7.94188...   \n",
      "1      [8.235803604125977, 8.206790924072266, 7.84517...   \n",
      "2      [8.030133247375488, 8.012803077697754, 7.98151...   \n",
      "3      [8.63869857788086, 8.119754791259766, 7.925565...   \n",
      "4      [8.026514053344727, 8.017280578613281, 7.98747...   \n",
      "...                                                  ...   \n",
      "18478  [8.474267959594727, 8.248497009277344, 8.02881...   \n",
      "18479  [8.476810455322266, 7.880990982055664, 7.82718...   \n",
      "18480  [8.778240203857422, 8.005950927734375, 7.82399...   \n",
      "18481  [8.028709411621094, 8.01646900177002, 7.954902...   \n",
      "18482  [8.787713050842285, 8.161685943603516, 7.89694...   \n",
      "\n",
      "                                           top6_max_prob  \\\n",
      "0      [0.11987905949354172, 0.12057046592235565, 0.1...   \n",
      "1      [0.11906196922063828, 0.12101028114557266, 0.1...   \n",
      "2      [0.1214059591293335, 0.12152682989835739, 0.12...   \n",
      "3      [0.1209755465388298, 0.12110031396150589, 0.12...   \n",
      "4      [0.12136172503232956, 0.12144865095615387, 0.1...   \n",
      "...                                                  ...   \n",
      "18478  [0.11867936700582504, 0.11954998224973679, 0.1...   \n",
      "18479  [0.11977476626634598, 0.12072121351957321, 0.1...   \n",
      "18480  [0.12017423659563065, 0.12128829210996628, 0.1...   \n",
      "18481  [0.11919179558753967, 0.11983215063810349, 0.1...   \n",
      "18482  [0.11849693953990936, 0.11939428746700287, 0.1...   \n",
      "\n",
      "                                        top6_min_logprob  \\\n",
      "0      [2.121271848678589, 2.11552095413208, 2.108742...   \n",
      "1      [2.1281111240386963, 2.111879825592041, 2.1041...   \n",
      "2      [2.1086153984069824, 2.1076202392578125, 2.105...   \n",
      "3      [2.1121668815612793, 2.111135959625244, 2.1089...   \n",
      "4      [2.1089797019958496, 2.1082637310028076, 2.095...   \n",
      "...                                                  ...   \n",
      "18478  [2.1313297748565674, 2.12402081489563, 2.11451...   \n",
      "18479  [2.1221423149108887, 2.114271402359009, 2.1129...   \n",
      "18480  [2.1188125610351562, 2.1095850467681885, 2.106...   \n",
      "18481  [2.127021312713623, 2.1216633319854736, 2.1148...   \n",
      "18482  [2.1328680515289307, 2.125324010848999, 2.1158...   \n",
      "\n",
      "                                        top6_max_entropy  \n",
      "0      [3.855966091156006, 3.854945182800293, 3.85476...  \n",
      "1      [3.8494982719421387, 3.8493337631225586, 3.848...  \n",
      "2      [3.8565139770507812, 3.856123447418213, 3.8559...  \n",
      "3      [3.855656623840332, 3.855259895324707, 3.85358...  \n",
      "4      [3.856593370437622, 3.8547847270965576, 3.8546...  \n",
      "...                                                  ...  \n",
      "18478  [3.8411965370178223, 3.8398079872131348, 3.837...  \n",
      "18479  [3.845726728439331, 3.8439528942108154, 3.8392...  \n",
      "18480  [3.842477321624756, 3.8417458534240723, 3.8414...  \n",
      "18481  [3.8406553268432617, 3.840304136276245, 3.8395...  \n",
      "18482  [3.839313507080078, 3.8379921913146973, 3.8352...  \n",
      "\n",
      "[18483 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "predict_results = anomaly_detector.predict(test_features)\n",
    "print (predict_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
