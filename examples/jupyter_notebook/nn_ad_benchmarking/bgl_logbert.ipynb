{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Anomaly Detection on BGL dataset using LogBERT model\n",
    "This is a running example of an end-to-end workflow of Log Anomaly Detection on public dataset HDFS using the LogBERT model.\n",
    "\n",
    "There are similar workflows on the BGL datasets using other anomaly detectors (like LSTM based one in `bgl_lstm_unsupervised_parsed_sequential.ipynb`). \n",
    "\n",
    "The actual workflow script is exactly identical in these cases, except in the LogBERT case we choose to skip the log-parsing step. This is simply done following past literature, but there are no restrictions from the LogAI library side. \n",
    "\n",
    "\n",
    "Also check out the other config files that in this directory that cater to other datasets (HDFS), or other experimental configs like (parsing/nonparsing based, sliding/session window based log partitioning, sequential/semantic log feature representations, supervised/unsupervised setting, LSTM/CNN/Transformer/BERT model). \n",
    "\n",
    "To use these different experimental configs, you only need to point to the correct config file and the same workflow code should work perfectly for those!\n",
    "\n",
    "Only in case of changing the dataset (eg. from BGL to HDFS) you need to not only change the config.yaml file but also use the HDFSPreprocessor in the preprocessing step. Note that each custom dataset that are added should have its own Preprocessor class (which should inherit from logai.preproces.preprocessor.Preprocessor). \n",
    "\n",
    "For more complete explanations of each step of the workflow check out the `hdfs_lstm_unsupervised_parsed_sequential.ipynb` notebook instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from logai.applications.openset.anomaly_detection.openset_anomaly_detection_workflow import OpenSetADWorkflowConfig, validate_config_dict\n",
    "from logai.utils.file_utils import read_file\n",
    "from logai.utils.dataset_utils import split_train_dev_test_for_anomaly_detection\n",
    "import logging \n",
    "from logai.dataloader.data_loader import FileDataLoader\n",
    "from logai.preprocess.bgl_preprocessor import BGLPreprocessor\n",
    "from logai.information_extraction.log_parser import LogParser\n",
    "from logai.preprocess.openset_partitioner import OpenSetPartitioner\n",
    "from logai.analysis.nn_anomaly_detector import NNAnomalyDetector\n",
    "from logai.information_extraction.log_vectorizer import LogVectorizer\n",
    "from logai.utils import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"bgl_logbert_config.yaml\"\n",
    "config_parsed = read_file(config_path)\n",
    "config_dict = config_parsed[\"workflow_config\"]\n",
    "config = OpenSetADWorkflowConfig.from_dict(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         RAS KERNEL INFO instruction cache parity error...\n",
      "1         RAS KERNEL INFO instruction cache parity error...\n",
      "2         RAS KERNEL INFO instruction cache parity error...\n",
      "3         RAS KERNEL INFO instruction cache parity error...\n",
      "4         RAS KERNEL INFO instruction cache parity error...\n",
      "                                ...                        \n",
      "358455    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358456    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358457    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358458    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358459    RAS KERNEL FATAL idoproxy communication failur...\n",
      "Name: logline, Length: 358460, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/logai/dataloader/data_loader.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected[constants.LOG_TIMESTAMPS] = pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "dataloader = FileDataLoader(config.data_loader_config)\n",
    "logrecord = dataloader.load_data()\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         RAS KERNEL INFO instruction cache parity error...\n",
      "1         RAS KERNEL INFO instruction cache parity error...\n",
      "2         RAS KERNEL INFO instruction cache parity error...\n",
      "3         RAS KERNEL INFO instruction cache parity error...\n",
      "4         RAS KERNEL INFO instruction cache parity error...\n",
      "                                ...                        \n",
      "358455    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358456    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358457    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358458    RAS KERNEL FATAL idoproxy communication failur...\n",
      "358459    RAS KERNEL FATAL idoproxy communication failur...\n",
      "Name: logline, Length: 358460, dtype: object\n"
     ]
    }
   ],
   "source": [
    "preprocessor = BGLPreprocessor(config.preprocessor_config)\n",
    "preprocessed_filepath = os.path.join(config.output_dir, 'BGL_11k_processed.csv')            \n",
    "logrecord = preprocessor.clean_log(logrecord)\n",
    "logrecord.save_to_csv(preprocessed_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       RAS KERNEL INFO instruction cache parity error...\n",
      "1       RAS KERNEL INFO instruction cache parity error...\n",
      "2       RAS KERNEL INFO instruction cache parity error...\n",
      "3       RAS KERNEL INFO instruction cache parity error...\n",
      "4       RAS KERNEL INFO instruction cache parity error...\n",
      "                              ...                        \n",
      "1848    RAS APP FATAL ciod Error reading message prefi...\n",
      "1849    RAS KERNEL FATAL Lustre mount FAILED ALPHANUM ...\n",
      "1850    RAS KERNEL FATAL Lustre mount FAILED ALPHANUM ...\n",
      "1851    RAS KERNEL FATAL Lustre mount FAILED ALPHANUM ...\n",
      "1852    RAS KERNEL FATAL idoproxy communication failur...\n",
      "Name: logline, Length: 1853, dtype: object\n"
     ]
    }
   ],
   "source": [
    "partitioner = OpenSetPartitioner(config.open_set_partitioner_config)\n",
    "partitioned_filepath = os.path.join(config.output_dir, 'BGL_11k_nonparsed_session.csv')\n",
    "logrecord = partitioner.partition(logrecord)\n",
    "logrecord.save_to_csv(partitioned_filepath)\n",
    "print (logrecord.body[constants.LOGLINE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_train/dev/test:  40 5 1808\n",
      "Train/Dev/Test Anomalous 0 0 1808\n",
      "Train/Dev/Test Normal 40 5 0\n"
     ]
    }
   ],
   "source": [
    "train_filepath = os.path.join(config.output_dir, 'BGL_11k_nonparsed_session_unsupervised_train.csv')\n",
    "dev_filepath = os.path.join(config.output_dir, 'BGL_11k_nonparsed_session_unsupervised_dev.csv')\n",
    "test_filepath = os.path.join(config.output_dir, 'BGL_11k_nonparsed_session_unsupervised_test.csv')\n",
    "\n",
    "(train_data, dev_data, test_data) = split_train_dev_test_for_anomaly_detection(\n",
    "                logrecord,training_type=config.training_type,\n",
    "                test_data_frac_neg_class=config.test_data_frac_neg,\n",
    "                test_data_frac_pos_class=config.test_data_frac_pos,\n",
    "                shuffle=config.train_test_shuffle\n",
    "            )\n",
    "\n",
    "train_data.save_to_csv(train_filepath)\n",
    "dev_data.save_to_csv(dev_filepath)\n",
    "test_data.save_to_csv(test_filepath)\n",
    "print ('Train/Dev/Test Anomalous', len(train_data.labels[train_data.labels[constants.LABELS]==1]), \n",
    "                                   len(dev_data.labels[dev_data.labels[constants.LABELS]==1]), \n",
    "                                   len(test_data.labels[test_data.labels[constants.LABELS]==1]))\n",
    "print ('Train/Dev/Test Normal', len(train_data.labels[train_data.labels[constants.LABELS]==0]), \n",
    "                                   len(dev_data.labels[dev_data.labels[constants.LABELS]==0]), \n",
    "                                   len(test_data.labels[test_data.labels[constants.LABELS]==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6d28105e6240c9bfa1fb4685250876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46468938d91340eb8e358560ee56e4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06671dce344b4d6eb0ba9622018fd977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c72a5c7eae4347ae09345bb294e561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef08c14d82664cea9286f589e5456d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b2eff6f09d4cb0b9a55452f5cb060f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c01779fc9e47519fb8d49988736206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541eaaa1093f410e8e8476108127793c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a562b1628934aaa86d2a1e5dca11d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a3aba2ca63404e950a4eef3432e277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac55600d594487cbcf37a9e700a350b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b569d10f074b2c96effbce8c7233dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 40\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "vectorizer = LogVectorizer(config.log_vectorizer_config)\n",
    "vectorizer.fit(train_data)\n",
    "train_features = vectorizer.transform(train_data)\n",
    "dev_features = vectorizer.transform(dev_data)\n",
    "test_features = vectorizer.transform(test_data)\n",
    "print (train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized data collator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 40\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:15, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.407800</td>\n",
       "      <td>7.452397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.909800</td>\n",
       "      <td>3.440429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-50\n",
      "Configuration saved in temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-50/config.json\n",
      "Model weights saved in temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-50/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100\n",
      "Configuration saved in temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100/config.json\n",
      "Model weights saved in temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anomaly_detector = NNAnomalyDetector(config=config.nn_anomaly_detection_config)\n",
    "anomaly_detector.fit(train_features, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model from /Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/examples/jupyter_notebook/nn_ad_benchmarking/temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100\n",
      "loading configuration file /Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/examples/jupyter_notebook/nn_ad_benchmarking/temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 332\n",
      "}\n",
      "\n",
      "loading weights file /Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/examples/jupyter_notebook/nn_ad_benchmarking/temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at /Users/amrita.saha/Home/salesforce/workspace/code/AIOps/RCA_Log/logai_opensource/logai/examples/jupyter_notebook/nn_ad_benchmarking/temp_output/BGL_11k_parsed_session_supervised_AD/bert-base-cased/checkpoint-100.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c744af9b70d42908ac8065ceadd75e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1808 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1849\n",
      "  Batch size = 256\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test_loss: 6.922546863555908 test_runtime: 390.1582 test_samples/s: 4.739\n",
      "INFO:root:number of original test instances 1471\n",
      "INFO:root:loss_mean Pos scores:  mean: 6.942281161443237, std: 0.774283953001779\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_max Pos scores:  mean: 8.484454650606613, std: 0.3959560440390038\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:loss_top6_mean Pos scores:  mean: 7.668759094596368, std: 0.5011946579908295\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_prob Pos scores:  mean: 0.8799232241718502, std: 0.0035729415337627285\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_min_logprob Pos scores:  mean: 2.120101489503179, std: 0.030195267421847985\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "INFO:root:scores_top6_max_entropy Pos scores:  mean: 3.835382603974994, std: 0.04770405087132924\n",
      "/Users/amrita.saha/opt/anaconda3/envs/loglib/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1020: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "INFO:root:\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1849\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/8 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_results = anomaly_detector.predict(test_features)\n",
    "print (predict_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
