{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from logai.dataloader.data_loader import FileDataLoader\n",
                "from logai.dataloader.data_loader import FileConfig\n",
                "from logai.information_extraction.log_parser import LogParser, LogParserConfig\n",
                "from logai.algorithms.parsing_algo.drain import DrainConfig\n",
                "from logai.preprocess.preprocess import Preprocess",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "outputs": [],
            "source": [
                "# Data layer\n",
                "# Load log data and store all data in @LogRecordObject. Currently only implemented FileDataLoader.\n",
                "\n",
                "# Please change the filepath correspondingly.\n",
                "# I've put the ./data dir in .gitignore to avoid checking in data unexpectedly\n",
                "\n",
                "#File Configuration\n",
                'filepath = "./data/mixed.csv"\n',
                "log_type = 'csv'\n",
                "dimensions = {'timestamp': ['timestamp'],\n",
                "              'attributes': ['cluster_count',\n",
                "                             'cluster_label',\n",
                "                             'logRecordType'],\n",
                "              'body': ['_raw']}\n",
                'custom_delimeter_regex = r"`+|\\s+"\n',
                "\n",
                "file_config = FileConfig(filepath=filepath, log_type='csv', dimensions=dimensions, custom_delimeter_regex=custom_delimeter_regex)\n",
                "\n",
                "\n",
                "dataloader = FileDataLoader()\n",
                "logrecord = dataloader.load_data(file_config)",
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "outputs": [
                {
                    "data": {
                        "text/plain": "   cluster_count  cluster_label logRecordType\n0            425             44         ffgen\n1             82            206         phqry\n2             32            328         phqry\n3           3556             17         mqdbg\n4           1360             16         mqdbg",
                        "text/html": '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>cluster_count</th>\n      <th>cluster_label</th>\n      <th>logRecordType</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>425</td>\n      <td>44</td>\n      <td>ffgen</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>82</td>\n      <td>206</td>\n      <td>phqry</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>32</td>\n      <td>328</td>\n      <td>phqry</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3556</td>\n      <td>17</td>\n      <td>mqdbg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1360</td>\n      <td>16</td>\n      <td>mqdbg</td>\n    </tr>\n  </tbody>\n</table>\n</div>',
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result",
                }
            ],
            "source": ["logrecord.attributes.head(5)"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess\n",
                "# Do customer rules to initially parse the loglines. Add custom delimeters in a regex\n",
                "# Group log records by any attributes. Return grouped log index so follow up process can handle them separately.\n",
                "\n",
                "\n",
                "preprocessed_loglines = Preprocess.clean_log(logrecord.body, file_config.custom_delimeter_regex)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "outputs": [
                {
                    "data": {
                        "text/plain": "                                                _raw\n0  ffgen 20210501000000.005 112100 0 0 acbcfa25dd...\n1  phqry 20210501000000.035 5436 0 0 28e3e6101f0d...\n2  phqry 20210501000000.121 1259 0 0 0 1o:035 232...\n3  mqdbg 20210501000000.124 1411 0 0 0 Qr:021 232...\n4  mqdbg 20210501000000.127 1411 0 0 0 fr:021 232...\n5  ffgen 20210501000000.159 87443 0 0 6ef02bafd13...\n6  ffgen 20210501000000.160 4bf7FOabZfbDZ4mt-SUBZ...\n7  phqry 20210501000000.174 5915 0 0 0 5-:035 232...\n8  phqry 20210501000000.177 5915 0 0 0 9-:035 232...\n9  ppcmi 20210501000000.195 4bf7FOh9O6dGPKmt-SUG-...",
                        "text/html": '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>_raw</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ffgen 20210501000000.005 112100 0 0 acbcfa25dd...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>phqry 20210501000000.035 5436 0 0 28e3e6101f0d...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>phqry 20210501000000.121 1259 0 0 0 1o:035 232...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mqdbg 20210501000000.124 1411 0 0 0 Qr:021 232...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mqdbg 20210501000000.127 1411 0 0 0 fr:021 232...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ffgen 20210501000000.159 87443 0 0 6ef02bafd13...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ffgen 20210501000000.160 4bf7FOabZfbDZ4mt-SUBZ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>phqry 20210501000000.174 5915 0 0 0 5-:035 232...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>phqry 20210501000000.177 5915 0 0 0 9-:035 232...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ppcmi 20210501000000.195 4bf7FOh9O6dGPKmt-SUG-...</td>\n    </tr>\n  </tbody>\n</table>\n</div>',
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result",
                }
            ],
            "source": ["preprocessed_loglines.head(10)"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Information Extraction\n",
                "\n",
                "# Advanced parsing: logline -> pd.Series\n",
                "# DRAIN: tree structure parsing algorithms, could be very slow when the log size is large.\n",
                "drain_config = DrainConfig(sim_th=0.2,\n",
                "                           extra_delimiters=[])\n",
                "\n",
                "log_parser_config = LogParserConfig(parsing_algorithm='drain',\n",
                "                                    parsing_algo_params=drain_config)\n",
                "\n",
                "parser = LogParser(log_parser_config)\n",
                "\n",
                "parsed_result = parser.parse(preprocessed_loglines[dimensions['body'][0]])",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "dict_keys(['log_cluster_depth', 'max_node_depth', 'sim_th', 'max_children', 'root_node', 'profiler', 'extra_delimiters', 'max_clusters', 'param_str', 'id_to_cluster', 'clusters_counter'])"
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result",
                }
            ],
            "source": [
                "from logai.algorithms.parsing_algo.drain import Drain\n",
                "parser = Drain()\n",
                "parser.__dict__.keys()",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "outputs": [
                {
                    "data": {"text/plain": "{'sim_th': 0.2, 'extra_delimiters': []}"},
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result",
                }
            ],
            "source": ["drain_config.__dict__"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "outputs": [],
            "source": [
                "#bucket loglines into groups.\n",
                "index_groups = Preprocess.group_log_index(logrecord.attributes, by=['cluster_label', 'logRecordType'])\n",
                "for index, row in index_groups.iterrows():\n",
                "    index_list = index_groups['group_index'].iloc[index]\n",
                "    if len(index_list) < 100:\n",
                "        continue\n",
                "    loglines = preprocessed_loglines.iloc[index_list]",
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "outputs": [],
            "source": [
                "## Vectorization\n",
                "\n",
                "# Below block is to solve the SSL verification issue when\n",
                "# NLTK tries to download required datasets to perform embedding\n",
                "# =================\n",
                "# import ssl\n",
                "#\n",
                "# try:\n",
                "#     _create_unverified_https_context = ssl._create_unverified_context\n",
                "# except AttributeError:\n",
                "#     pass\n",
                "# else:\n",
                "#     ssl._create_default_https_context = _create_unverified_https_context\n",
                "#\n",
                "# nltk.download()\n",
                "# ==================\n",
                "#\n",
                "\n",
                "\n",
                "from information_extraction.log_vectorizer import LogVectorizer\n",
                "\n",
                "vectorizor = LogVectorizer()\n",
                "\n",
                "parsed_loglines = parsed_result['parsed_logline']\n",
                "\n",
                "vectorizor.fit(parsed_loglines)\n",
                "\n",
                "#Log vector is a pandas.Series\n",
                "log_vectors = vectorizor.transform(parsed_loglines)",
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": 170,
            "outputs": [
                {
                    "ename": "ImportError",
                    "evalue": "cannot import name 'FeatureExtractorConfig' from 'information_extraction.feature_extractor' (/Users/qcheng/workspace/gitsoma/logai/logai/information_extraction/feature_extractor.py)",
                    "output_type": "error",
                    "traceback": [
                        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
                        "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
                        "\u001B[0;32m/var/folders/gm/zf03v70n4ndcrg17spcgnrk80000gq/T/ipykernel_22939/242435191.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpreprocessing\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mOrdinalEncoder\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0minformation_extraction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeature_extractor\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mFeatureExtractor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mFeatureExtractorConfig\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0mconfig\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mFeatureExtractorConfig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgroup_by_category\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'cluster_label'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'logRecordType'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
                        "\u001B[0;31mImportError\u001B[0m: cannot import name 'FeatureExtractorConfig' from 'information_extraction.feature_extractor' (/Users/qcheng/workspace/gitsoma/logai/logai/information_extraction/feature_extractor.py)",
                    ],
                }
            ],
            "source": [
                "import dataclasses\n",
                "from dataclasses import dataclass, fields\n",
                "#Feature extraction\n",
                "# implement log vector to feature\n",
                "# this will convert the vector metrics to a n dimensional feature.\n",
                "# implement simple 0 padding method.\n",
                "\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import OrdinalEncoder\n",
                "from information_extraction.feature_extractor import FeatureExtractor, FeatureExtractorConfig\n",
                "\n",
                "config = FeatureExtractorConfig(group_by_category=['cluster_label', 'logRecordType'])\n",
                "\n",
                "max_len = 300\n",
                "feature_extractor = FeatureExtractor(config)\n",
                "attributes = logrecord.attributes\n",
                "timestamps = pd.to_datetime(logrecord.timestamp['timestamp'])\n",
                "feature_vector = feature_extractor.convert_to_feature_vector(log_vectors, attributes, timestamps, max_len)\n",
                "\n",
                "# Counter vector on string pattern\n",
                "counter_vector_str = feature_extractor.convert_to_counter_vector(parsed_loglines, attributes, timestamps)\n",
                "\n",
                "# Counter vector on log vector. Convert np array to string.\n",
                "log_vector_pattern = log_vectors.apply(lambda x: np.array2string(x, formatter={'float_kind':lambda x: \"%.2f\" % x})).rename('log_pattern')\n",
                "counter_vector_numeric = feature_extractor.convert_to_counter_vector(log_vector_pattern, attributes, timestamps)",
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": 166,
            "outputs": [],
            "source": [],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": 153,
            "outputs": [
                {
                    "data": {
                        "text/plain": "       0  cluster_count  cluster_label logRecordType  group_index  \\\n0                   425             44         ffgen            0   \n1                    82            206         phqry            1   \n2                    32            328         phqry            2   \n3                  3556             17         mqdbg            3   \n4                  1360             16         mqdbg            4   \n...   ..            ...            ...           ...          ...   \n86930              3378             36         mlmul        86930   \n86931                17            984         phqry        86931   \n86932                63            193         ffgen        86932   \n86933                41            103         phqry        86933   \n86934              3792             65         mqdbg        86934   \n\n                             timestamp  \n0     2021-05-01 00:00:00.005000+00:00  \n1     2021-05-01 00:00:00.035000+00:00  \n2     2021-05-01 00:00:00.121000+00:00  \n3     2021-05-01 00:00:00.124000+00:00  \n4     2021-05-01 00:00:00.127000+00:00  \n...                                ...  \n86930 2021-05-01 00:14:59.919000+00:00  \n86931 2021-05-01 00:14:59.940000+00:00  \n86932 2021-05-01 00:14:59.943000+00:00  \n86933 2021-05-01 00:14:59.951000+00:00  \n86934 2021-05-01 00:14:59.983000+00:00  \n\n[86935 rows x 6 columns]",
                        "text/html": '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>0</th>\n      <th>cluster_count</th>\n      <th>cluster_label</th>\n      <th>logRecordType</th>\n      <th>group_index</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td>425</td>\n      <td>44</td>\n      <td>ffgen</td>\n      <td>0</td>\n      <td>2021-05-01 00:00:00.005000+00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td></td>\n      <td>82</td>\n      <td>206</td>\n      <td>phqry</td>\n      <td>1</td>\n      <td>2021-05-01 00:00:00.035000+00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td></td>\n      <td>32</td>\n      <td>328</td>\n      <td>phqry</td>\n      <td>2</td>\n      <td>2021-05-01 00:00:00.121000+00:00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td></td>\n      <td>3556</td>\n      <td>17</td>\n      <td>mqdbg</td>\n      <td>3</td>\n      <td>2021-05-01 00:00:00.124000+00:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td></td>\n      <td>1360</td>\n      <td>16</td>\n      <td>mqdbg</td>\n      <td>4</td>\n      <td>2021-05-01 00:00:00.127000+00:00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>86930</th>\n      <td></td>\n      <td>3378</td>\n      <td>36</td>\n      <td>mlmul</td>\n      <td>86930</td>\n      <td>2021-05-01 00:14:59.919000+00:00</td>\n    </tr>\n    <tr>\n      <th>86931</th>\n      <td></td>\n      <td>17</td>\n      <td>984</td>\n      <td>phqry</td>\n      <td>86931</td>\n      <td>2021-05-01 00:14:59.940000+00:00</td>\n    </tr>\n    <tr>\n      <th>86932</th>\n      <td></td>\n      <td>63</td>\n      <td>193</td>\n      <td>ffgen</td>\n      <td>86932</td>\n      <td>2021-05-01 00:14:59.943000+00:00</td>\n    </tr>\n    <tr>\n      <th>86933</th>\n      <td></td>\n      <td>41</td>\n      <td>103</td>\n      <td>phqry</td>\n      <td>86933</td>\n      <td>2021-05-01 00:14:59.951000+00:00</td>\n    </tr>\n    <tr>\n      <th>86934</th>\n      <td></td>\n      <td>3792</td>\n      <td>65</td>\n      <td>mqdbg</td>\n      <td>86934</td>\n      <td>2021-05-01 00:14:59.983000+00:00</td>\n    </tr>\n  </tbody>\n</table>\n<p>86935 rows × 6 columns</p>\n</div>',
                    },
                    "execution_count": 153,
                    "metadata": {},
                    "output_type": "execute_result",
                }
            ],
            "source": ["counter_vector"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": 164,
            "outputs": [
                {
                    "data": {
                        "text/plain": "                                             log_pattern  cluster_count  \\\n0      [-0.24 -0.08 -0.06 -0.02 0.01 0.17 -0.02 0.01 ...            425   \n1      [-0.29 -0.15 -0.00 -0.02 0.01 0.17 -0.02 0.01 ...             82   \n2      [-0.29 -0.15 -0.00 -0.02 0.01 0.17 -0.02 0.01 ...             32   \n3      [-0.01 -0.26 0.32 -0.02 0.01 0.17 -0.02 0.01 0...           3556   \n4      [-0.01 -0.26 0.32 -0.02 0.01 0.17 -0.02 0.01 0...           1360   \n...                                                  ...            ...   \n86930  [-0.16 -0.32 0.17 -0.02 0.01 0.17 -0.02 0.01 0...           3378   \n86931  [-0.29 -0.15 -0.00 -0.02 0.01 0.17 -0.02 0.01 ...             17   \n86932  [-0.24 -0.08 -0.06 -0.02 0.01 0.17 -0.02 0.01 ...             63   \n86933  [-0.29 -0.15 -0.00 -0.02 0.01 0.17 -0.02 0.01 ...             41   \n86934  [-0.01 -0.26 0.32 -0.02 0.01 0.17 -0.02 0.01 0...           3792   \n\n       cluster_label logRecordType  group_index  \\\n0                 44         ffgen            0   \n1                206         phqry            1   \n2                328         phqry            2   \n3                 17         mqdbg            3   \n4                 16         mqdbg            4   \n...              ...           ...          ...   \n86930             36         mlmul        86930   \n86931            984         phqry        86931   \n86932            193         ffgen        86932   \n86933            103         phqry        86933   \n86934             65         mqdbg        86934   \n\n                             timestamp  \n0     2021-05-01 00:00:00.005000+00:00  \n1     2021-05-01 00:00:00.035000+00:00  \n2     2021-05-01 00:00:00.121000+00:00  \n3     2021-05-01 00:00:00.124000+00:00  \n4     2021-05-01 00:00:00.127000+00:00  \n...                                ...  \n86930 2021-05-01 00:14:59.919000+00:00  \n86931 2021-05-01 00:14:59.940000+00:00  \n86932 2021-05-01 00:14:59.943000+00:00  \n86933 2021-05-01 00:14:59.951000+00:00  \n86934 2021-05-01 00:14:59.983000+00:00  \n\n[86935 rows x 6 columns]",
                        "text/html": '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>log_pattern</th>\n      <th>cluster_count</th>\n      <th>cluster_label</th>\n      <th>logRecordType</th>\n      <th>group_index</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-0.24 -0.08 -0.06 -0.02 0.01 0.17 -0.02 0.01 ...</td>\n      <td>425</td>\n      <td>44</td>\n      <td>ffgen</td>\n      <td>0</td>\n      <td>2021-05-01 00:00:00.005000+00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-0.29 -0.15 -0.00 -0.02 0.01 0.17 -0.02 0.01 ...</td>\n      <td>82</td>\n      <td>206</td>\n      <td>phqry</td>\n      <td>1</td>\n      <td>2021-05-01 00:00:00.035000+00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[-0.29 -0.15 -0.00 -0.02 0.01 0.17 -0.02 0.01 ...</td>\n      <td>32</td>\n      <td>328</td>\n      <td>phqry</td>\n      <td>2</td>\n      <td>2021-05-01 00:00:00.121000+00:00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[-0.01 -0.26 0.32 -0.02 0.01 0.17 -0.02 0.01 0...</td>\n      <td>3556</td>\n      <td>17</td>\n      <td>mqdbg</td>\n      <td>3</td>\n      <td>2021-05-01 00:00:00.124000+00:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[-0.01 -0.26 0.32 -0.02 0.01 0.17 -0.02 0.01 0...</td>\n      <td>1360</td>\n      <td>16</td>\n      <td>mqdbg</td>\n      <td>4</td>\n      <td>2021-05-01 00:00:00.127000+00:00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>86930</th>\n      <td>[-0.16 -0.32 0.17 -0.02 0.01 0.17 -0.02 0.01 0...</td>\n      <td>3378</td>\n      <td>36</td>\n      <td>mlmul</td>\n      <td>86930</td>\n      <td>2021-05-01 00:14:59.919000+00:00</td>\n    </tr>\n    <tr>\n      <th>86931</th>\n      <td>[-0.29 -0.15 -0.00 -0.02 0.01 0.17 -0.02 0.01 ...</td>\n      <td>17</td>\n      <td>984</td>\n      <td>phqry</td>\n      <td>86931</td>\n      <td>2021-05-01 00:14:59.940000+00:00</td>\n    </tr>\n    <tr>\n      <th>86932</th>\n      <td>[-0.24 -0.08 -0.06 -0.02 0.01 0.17 -0.02 0.01 ...</td>\n      <td>63</td>\n      <td>193</td>\n      <td>ffgen</td>\n      <td>86932</td>\n      <td>2021-05-01 00:14:59.943000+00:00</td>\n    </tr>\n    <tr>\n      <th>86933</th>\n      <td>[-0.29 -0.15 -0.00 -0.02 0.01 0.17 -0.02 0.01 ...</td>\n      <td>41</td>\n      <td>103</td>\n      <td>phqry</td>\n      <td>86933</td>\n      <td>2021-05-01 00:14:59.951000+00:00</td>\n    </tr>\n    <tr>\n      <th>86934</th>\n      <td>[-0.01 -0.26 0.32 -0.02 0.01 0.17 -0.02 0.01 0...</td>\n      <td>3792</td>\n      <td>65</td>\n      <td>mqdbg</td>\n      <td>86934</td>\n      <td>2021-05-01 00:14:59.983000+00:00</td>\n    </tr>\n  </tbody>\n</table>\n<p>86935 rows × 6 columns</p>\n</div>',
                    },
                    "execution_count": 164,
                    "metadata": {},
                    "output_type": "execute_result",
                }
            ],
            "source": [
                "pd.concat((log_vectors.apply(lambda x: np.array2string(x, formatter={'float_kind':lambda x: \"%.2f\" % x})).rename('log_pattern'), attributes, timestamps), axis=1)"
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": 115,
            "outputs": [],
            "source": [
                "config = FeatureExtractorConfig(group_by_category=['cluster_label', 'logRecordType'], group_by_time=\"1min\")\n",
                "feature_extractor = FeatureExtractor(config)\n",
                "\n",
                "attributes = logrecord.attributes\n",
                "timestamps = pd.to_datetime(logrecord.timestamp['timestamp'])\n",
                "log_pattern = parsed_loglines\n",
                "\n",
                "if feature_extractor.group_by_category:\n",
                "    feature_df = pd.concat((log_pattern, attributes[feature_extractor.group_by_category]), axis=1)\n",
                "\n",
                "if feature_extractor.group_by_time:\n",
                "    truncated_ts = timestamps.dt.floor(freq=feature_extractor.group_by_time)\n",
                "    feature_df = pd.concat((feature_df, truncated_ts), axis=1)\n",
                "\n",
                "feature_df.value_counts().reset_index(name='counts')\n",
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": 130,
            "outputs": [
                {
                    "data": {
                        "text/plain": "                                          parsed_logline  cluster_label  \\\n0                       mlmul * * 0 0 * 232.3.3 INFO * *              3   \n1                       mlmul * * 0 0 * 232.3.3 INFO * *              3   \n2                       mlmul * * 0 0 * 232.3.3 INFO * *             94   \n3      ksgen * 1682 0 0 0 * 232.3.3 INFO Add Capacity...             11   \n4                       mlmul * * 0 0 * 232.3.3 INFO * *             59   \n...                                                  ...            ...   \n27254                   mlmul * * 0 0 * 232.3.3 INFO * *             59   \n27255                   mlmul * * 0 0 * 232.3.3 INFO * *             58   \n27256  ailtn * * * 0 0 * * * * 232.3.3 * * * * * * 23...           1361   \n27257                   mlmul * * 0 0 * 232.3.3 INFO * *             55   \n27258  s 20210501000832.805 1304 0 0 bC:034 232.3.3 2...           2284   \n\n      logRecordType                 timestamp  counts  \n0             mlmul 2021-05-01 00:04:00+00:00     802  \n1             mlmul 2021-05-01 00:06:00+00:00     443  \n2             mlmul 2021-05-01 00:04:00+00:00     398  \n3             ksgen 2021-05-01 00:03:00+00:00     397  \n4             mlmul 2021-05-01 00:06:00+00:00     365  \n...             ...                       ...     ...  \n27254         mlmul 2021-05-01 00:00:00+00:00       1  \n27255         mlmul 2021-05-01 00:12:00+00:00       1  \n27256         ailtn 2021-05-01 00:12:00+00:00       1  \n27257         mlmul 2021-05-01 00:07:00+00:00       1  \n27258             s 2021-05-01 00:08:00+00:00       1  \n\n[27259 rows x 5 columns]",
                        "text/html": '<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>parsed_logline</th>\n      <th>cluster_label</th>\n      <th>logRecordType</th>\n      <th>timestamp</th>\n      <th>counts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>mlmul * * 0 0 * 232.3.3 INFO * *</td>\n      <td>3</td>\n      <td>mlmul</td>\n      <td>2021-05-01 00:04:00+00:00</td>\n      <td>802</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mlmul * * 0 0 * 232.3.3 INFO * *</td>\n      <td>3</td>\n      <td>mlmul</td>\n      <td>2021-05-01 00:06:00+00:00</td>\n      <td>443</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>mlmul * * 0 0 * 232.3.3 INFO * *</td>\n      <td>94</td>\n      <td>mlmul</td>\n      <td>2021-05-01 00:04:00+00:00</td>\n      <td>398</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ksgen * 1682 0 0 0 * 232.3.3 INFO Add Capacity...</td>\n      <td>11</td>\n      <td>ksgen</td>\n      <td>2021-05-01 00:03:00+00:00</td>\n      <td>397</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mlmul * * 0 0 * 232.3.3 INFO * *</td>\n      <td>59</td>\n      <td>mlmul</td>\n      <td>2021-05-01 00:06:00+00:00</td>\n      <td>365</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27254</th>\n      <td>mlmul * * 0 0 * 232.3.3 INFO * *</td>\n      <td>59</td>\n      <td>mlmul</td>\n      <td>2021-05-01 00:00:00+00:00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27255</th>\n      <td>mlmul * * 0 0 * 232.3.3 INFO * *</td>\n      <td>58</td>\n      <td>mlmul</td>\n      <td>2021-05-01 00:12:00+00:00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27256</th>\n      <td>ailtn * * * 0 0 * * * * 232.3.3 * * * * * * 23...</td>\n      <td>1361</td>\n      <td>ailtn</td>\n      <td>2021-05-01 00:12:00+00:00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27257</th>\n      <td>mlmul * * 0 0 * 232.3.3 INFO * *</td>\n      <td>55</td>\n      <td>mlmul</td>\n      <td>2021-05-01 00:07:00+00:00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27258</th>\n      <td>s 20210501000832.805 1304 0 0 bC:034 232.3.3 2...</td>\n      <td>2284</td>\n      <td>s</td>\n      <td>2021-05-01 00:08:00+00:00</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>27259 rows × 5 columns</p>\n</div>',
                    },
                    "execution_count": 130,
                    "metadata": {},
                    "output_type": "execute_result",
                }
            ],
            "source": ["\n", "\n"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "# Build feature set from unstructured loglines\n",
                "vector_df = feature_extractor.convert_to_feature_vector(log_vectors, max_len=300)\n",
                "\n",
                "#Build feature set from structured log attributes\n",
                "ord_enc = OrdinalEncoder()\n",
                'logrecord.attributes["logRecordType_code"] = ord_enc.fit_transform(logrecord.attributes[["logRecordType"]])\n',
                "\n",
                "\n",
                "groups = ['cluster_label', 'logRecordType']\n",
                "\n",
                "#bucket loglines into groups.\n",
                "index_groups = Preprocess.group_log_index(logrecord.attributes, by=groups)\n",
                "\n",
                "\n",
                "#Integrate the feature set\n",
                "for ig in index_groups:\n",
                "    index_list = index_groups['group_index'].iloc[index]\n",
                "    vector_df.iloc[index_list]\n",
                "\n",
                'integrated_df = pd.concat((logrecord.attributes["logRecordType_code"], vector_df), axis=1)\n',
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["integrated_df.shape"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "integrated_df.index = pd.to_datetime(logrecord.timestamp['timestamp'])"
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["feature_vector = integrated_df.rolling(100).mean()"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["feature_vector.shape"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["\n", "#Clustering\n", "# implement DBSCAN\n", "\n", "\n"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "from sklearn.cluster import DBSCAN\n",
                "from sklearn import metrics\n",
                "from sklearn.datasets import make_blobs\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "model = DBSCAN(eps=0.3, min_samples=10)\n",
                "model.fit(integrated_df)",
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["res = model.fit_predict(integrated_df)"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "res_series = pd.Series(res, index = integrated_df.head(5).index)"
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["res_series"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
                "core_samples_mask[db.core_sample_indices_] = True\n",
                "labels = db.labels_\n",
                "\n",
                "# Number of clusters in labels, ignoring noise if present.\n",
                "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
                "n_noise_ = list(labels).count(-1)\n",
                "\n",
                'print("Estimated number of clusters: %d" % n_clusters_)\n',
                'print("Estimated number of noise points: %d" % n_noise_)\n',
                '#print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))\n',
                '# print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))\n',
                '# print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))\n',
                '# print("Adjusted Rand Index: %0.3f" % metrics.adjusted_rand_score(labels_true, labels))\n',
                "# print(\n",
                '#     "Adjusted Mutual Information: %0.3f"\n',
                "#     % metrics.adjusted_mutual_info_score(labels_true, labels)\n",
                "# )\n",
                '# print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(X, labels))',
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "cluster_ids = pd.Series(db.labels_, index=integrated_df.index)"
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "#cluster_ids.value_counts()\n",
                "\n",
                "db.fit_predict(integrated_df)",
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "from sklearn.cluster import KMeans\n",
                "\n",
                "# k-means\n",
                "kmeans = KMeans(100)\n",
                "kmeans.fit(integrated_df)",
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["res = kmeans.predict(integrated_df)"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["res\n"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["cluster_res = pd.Series(res, index=integrated_df.index)"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["cluster_ids.value_counts()"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "#parsed_result.head(10).to_csv('/Users/qcheng/workspace/gitsoma/logai/results/vectorization_Result.txt')"
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "# Vectorization\n",
                "# TF-IDF on parsed loglines\n",
                "\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                '#doc = [" ".join(pl) for pl in parsed_log]\n',
                "doc = parsed_log\n",
                "vectorizer = TfidfVectorizer()\n",
                "vectorizer.fit(doc)\n",
                "\n",
                "X = vectorizer.transform(doc)",
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["vectorizer.get_feature_names_out()"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["X.toarray()"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["logrecord.attributes"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "#Integration\n",
                "\n",
                "integrated = logrecord.attributes.copy()\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "# #############################################################################\n",
                "# Generate sample data\n",
                "centers = [[1, 1], [-1, -1], [1, -1]]\n",
                "X, labels_true = make_blobs(\n",
                "    n_samples=750, centers=centers, cluster_std=0.4, random_state=0\n",
                ")\n",
                "\n",
                "X = StandardScaler().fit_transform(X)",
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["X[0]"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "# #############################################################################\n",
                "# Compute DBSCAN\n",
                "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
                "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
                "core_samples_mask[db.core_sample_indices_] = True\n",
                "labels = db.labels_\n",
                "\n",
                "# Number of clusters in labels, ignoring noise if present.\n",
                "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
                "n_noise_ = list(labels).count(-1)\n",
                "\n",
                'print("Estimated number of clusters: %d" % n_clusters_)\n',
                'print("Estimated number of noise points: %d" % n_noise_)\n',
                'print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))\n',
                'print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))\n',
                'print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))\n',
                'print("Adjusted Rand Index: %0.3f" % metrics.adjusted_rand_score(labels_true, labels))\n',
                "print(\n",
                '    "Adjusted Mutual Information: %0.3f"\n',
                "    % metrics.adjusted_mutual_info_score(labels_true, labels)\n",
                ")\n",
                'print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(X, labels))\n',
                "\n",
                "\n",
                "\n",
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["#Anomaly detection\n"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["timestamps = pd.to_datetime(logrecord.timestamp['timestamp'])"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": [
                "from logai.preprocess.preprocess import Preprocess\n",
                "\n",
                "index_group = Preprocess.group_log_index(logrecord.timestamp, by=['timestamp'])",
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["index_group.index = index_group['timestamp']"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["time_index = index_group['group_index']\n", "\n"],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'time_index' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
                        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
                        "\u001B[0;32m/var/folders/gm/zf03v70n4ndcrg17spcgnrk80000gq/T/ipykernel_22939/2536624896.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtime_index\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropna\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
                        "\u001B[0;31mNameError\u001B[0m: name 'time_index' is not defined",
                    ],
                }
            ],
            "source": [
                "time_index.dropna()\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                '"""\n',
                "(input pd.Series)\n",
                "index | array\n",
                "1:00  | [1,2,3]\n",
                "2:00  | [4,5,6]\n",
                "3:00  | [7,8,9]\n",
                "\n",
                "==> rolling window of 2 hours\n",
                "(expected)\n",
                "\n",
                "index | array\n",
                "1:00  | [1,2,3,4,5,6]\n",
                "2:00  | [4,5,6,7,8,9]\n",
                "\n",
                '"""',
            ],
            "metadata": {"collapsed": false, "pycharm": {"name": "#%%\n"}},
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "source": ["log_vectors.value_counts()\n"],
            "metadata": {
                "collapsed": false,
                "pycharm": {"name": "#%%\n", "is_executing": true},
            },
        },
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3",
        },
        "language_info": {
            "codemirror_mode": {"name": "ipython", "version": 3},
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7",
        },
    },
    "nbformat": 4,
    "nbformat_minor": 1,
}
